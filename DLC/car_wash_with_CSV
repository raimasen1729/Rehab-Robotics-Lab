{"cells":[{"cell_type":"markdown","metadata":{"id":"RK255E7YoEIt"},"source":["# DeepLabCut Toolbox - Colab for standard (single animal) projects!\n","https://github.com/DeepLabCut/DeepLabCut\n","\n","This notebook illustrates how to use the cloud to:\n","- create a training set\n","- train a network\n","- evaluate a network\n","- create simple quality check plots\n","- analyze novel videos!\n","\n","###This notebook assumes you already have a project folder with labeled data! \n","\n","This notebook demonstrates the necessary steps to use DeepLabCut for your own project.\n","\n","This shows the most simple code to do so, but many of the functions have additional features, so please check out the overview & the protocol paper!\n","\n","Nath\\*, Mathis\\* et al.: Using DeepLabCut for markerless pose estimation during behavior across species. Nature Protocols, 2019.\n","\n","\n","Paper: https://www.nature.com/articles/s41596-019-0176-0\n","\n","Pre-print: https://www.biorxiv.org/content/biorxiv/early/2018/11/24/476531.full.pdf\n"]},{"cell_type":"markdown","metadata":{"id":"txoddlM8hLKm"},"source":["## First, go to \"Runtime\" ->\"change runtime type\"->select \"Python3\", and then select \"GPU\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25482,"status":"ok","timestamp":1646689915416,"user":{"displayName":"Raima Sen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPtYpIHHudDQRAtyYm8EgdjpP9gnylWwSSjx-q=s64","userId":"00263744560502469226"},"user_tz":300},"id":"q23BzhA6CXxu","outputId":"d79cf967-a30f-49de-dece-ec795f56d9d1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting deeplabcut\n","  Downloading deeplabcut-2.2.0.6-py3-none-any.whl (544 kB)\n","\u001b[K     |████████████████████████████████| 544 kB 4.3 MB/s \n","\u001b[?25hCollecting tf-slim\n","  Downloading tf_slim-1.1.0-py2.py3-none-any.whl (352 kB)\n","\u001b[K     |████████████████████████████████| 352 kB 27.3 MB/s \n","\u001b[?25hRequirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (2.6.3)\n","Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.51.2)\n","Requirement already satisfied: tensorflow>=2.0 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (2.8.0)\n","Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (1.3.5)\n","Requirement already satisfied: scipy>=1.4 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (1.4.1)\n","Collecting ruamel.yaml>=0.15.0\n","  Downloading ruamel.yaml-0.17.21-py3-none-any.whl (109 kB)\n","\u001b[K     |████████████████████████████████| 109 kB 62.8 MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (3.2.2)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (5.5.0)\n","Collecting imgaug>=0.4.0\n","  Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n","\u001b[K     |████████████████████████████████| 948 kB 46.9 MB/s \n","\u001b[?25hCollecting tables<=3.6.1\n","  Downloading tables-3.6.1-cp37-cp37m-manylinux1_x86_64.whl (4.3 MB)\n","\u001b[K     |████████████████████████████████| 4.3 MB 55.2 MB/s \n","\u001b[?25hCollecting filterpy\n","  Downloading filterpy-1.4.5.zip (177 kB)\n","\u001b[K     |████████████████████████████████| 177 kB 57.0 MB/s \n","\u001b[?25hRequirement already satisfied: moviepy in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (0.2.3.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (3.13)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (1.0.2)\n","Collecting statsmodels>=0.11\n","  Downloading statsmodels-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n","\u001b[K     |████████████████████████████████| 9.8 MB 37.7 MB/s \n","\u001b[?25hCollecting tensorpack\n","  Downloading tensorpack-0.11-py2.py3-none-any.whl (296 kB)\n","\u001b[K     |████████████████████████████████| 296 kB 59.0 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (1.21.5)\n","Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (7.1.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from deeplabcut) (4.63.0)\n","Collecting scikit-image<=0.18.1,>=0.17\n","  Downloading scikit_image-0.18.1-cp37-cp37m-manylinux1_x86_64.whl (29.2 MB)\n","\u001b[K     |████████████████████████████████| 29.2 MB 1.2 MB/s \n","\u001b[?25hRequirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->deeplabcut) (1.8.1.post1)\n","Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->deeplabcut) (2.4.1)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->deeplabcut) (4.1.2.30)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->deeplabcut) (1.15.0)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->deeplabcut) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0.1->deeplabcut) (2018.9)\n","Collecting ruamel.yaml.clib>=0.2.6\n","  Downloading ruamel.yaml.clib-0.2.6-cp37-cp37m-manylinux1_x86_64.whl (546 kB)\n","\u001b[K     |████████████████████████████████| 546 kB 52.7 MB/s \n","\u001b[?25hRequirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image<=0.18.1,>=0.17->deeplabcut) (1.2.0)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image<=0.18.1,>=0.17->deeplabcut) (2021.11.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->deeplabcut) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->deeplabcut) (3.0.7)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->deeplabcut) (1.3.2)\n","Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.11->deeplabcut) (21.3)\n","Requirement already satisfied: patsy>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from statsmodels>=0.11->deeplabcut) (0.5.2)\n","Requirement already satisfied: numexpr>=2.6.2 in /usr/local/lib/python3.7/dist-packages (from tables<=3.6.1->deeplabcut) (2.8.1)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (3.17.3)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (13.0.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (57.4.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (3.10.0.2)\n","Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (2.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (0.24.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (0.2.0)\n","Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (0.5.3)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (1.13.3)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (3.3.0)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (1.0.0)\n","Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (2.8.0)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (1.44.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (1.6.3)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (1.1.0)\n","Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (2.8.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (3.1.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.0->deeplabcut) (1.1.2)\n","Collecting tf-estimator-nightly==2.8.0.dev2021122109\n","  Downloading tf_estimator_nightly-2.8.0.dev2021122109-py2.py3-none-any.whl (462 kB)\n","\u001b[K     |████████████████████████████████| 462 kB 61.1 MB/s \n","\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow>=2.0->deeplabcut) (0.37.1)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow>=2.0->deeplabcut) (1.5.2)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (1.35.0)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (0.4.6)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (1.8.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (2.23.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (3.3.6)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (0.6.1)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (4.2.4)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (0.2.8)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (4.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (4.11.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (3.7.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (0.4.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow>=2.0->deeplabcut) (3.2.0)\n","Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (0.8.1)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (4.4.2)\n","Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (2.6.1)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (4.8.0)\n","Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (1.0.18)\n","Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (5.1.1)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->deeplabcut) (0.7.5)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->deeplabcut) (0.2.5)\n","Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->deeplabcut) (0.34.0)\n","Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->deeplabcut) (0.7.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->deeplabcut) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->deeplabcut) (3.1.0)\n","Requirement already satisfied: pyzmq>=16 in /usr/local/lib/python3.7/dist-packages (from tensorpack->deeplabcut) (22.3.0)\n","Requirement already satisfied: psutil>=5 in /usr/local/lib/python3.7/dist-packages (from tensorpack->deeplabcut) (5.4.8)\n","Collecting msgpack-numpy>=0.4.4.2\n","  Downloading msgpack_numpy-0.4.7.1-py2.py3-none-any.whl (6.7 kB)\n","Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.7/dist-packages (from tensorpack->deeplabcut) (0.8.9)\n","Requirement already satisfied: msgpack>=0.5.2 in /usr/local/lib/python3.7/dist-packages (from tensorpack->deeplabcut) (1.0.3)\n","Building wheels for collected packages: filterpy\n","  Building wheel for filterpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for filterpy: filename=filterpy-1.4.5-py3-none-any.whl size=110474 sha256=716f7e25a7f58b9b644d4b77e8611ad36d59e8d56ddf2340b8da10426fbe9b85\n","  Stored in directory: /root/.cache/pip/wheels/ce/e0/ee/a2b3c5caab3418c1ccd8c4de573d4cbe13315d7e8b0a55fbc2\n","Successfully built filterpy\n","Installing collected packages: tf-estimator-nightly, scikit-image, ruamel.yaml.clib, msgpack-numpy, tf-slim, tensorpack, tables, statsmodels, ruamel.yaml, imgaug, filterpy, deeplabcut\n","  Attempting uninstall: scikit-image\n","    Found existing installation: scikit-image 0.18.3\n","    Uninstalling scikit-image-0.18.3:\n","      Successfully uninstalled scikit-image-0.18.3\n","  Attempting uninstall: tables\n","    Found existing installation: tables 3.7.0\n","    Uninstalling tables-3.7.0:\n","      Successfully uninstalled tables-3.7.0\n","  Attempting uninstall: statsmodels\n","    Found existing installation: statsmodels 0.10.2\n","    Uninstalling statsmodels-0.10.2:\n","      Successfully uninstalled statsmodels-0.10.2\n","  Attempting uninstall: imgaug\n","    Found existing installation: imgaug 0.2.9\n","    Uninstalling imgaug-0.2.9:\n","      Successfully uninstalled imgaug-0.2.9\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.4.0 which is incompatible.\u001b[0m\n","Successfully installed deeplabcut-2.2.0.6 filterpy-1.4.5 imgaug-0.4.0 msgpack-numpy-0.4.7.1 ruamel.yaml-0.17.21 ruamel.yaml.clib-0.2.6 scikit-image-0.18.1 statsmodels-0.13.2 tables-3.6.1 tensorpack-0.11 tf-estimator-nightly-2.8.0.dev2021122109 tf-slim-1.1.0\n"]}],"source":["#(this will take a few minutes to install all the dependences!)\n","!pip install deeplabcut"]},{"cell_type":"markdown","metadata":{"id":"25wSj6TlVclR"},"source":["**(Be sure to click \"RESTART RUNTIME\" is it is displayed above above before moving on !)**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":54,"status":"ok","timestamp":1646684622806,"user":{"displayName":"Raima Sen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPtYpIHHudDQRAtyYm8EgdjpP9gnylWwSSjx-q=s64","userId":"00263744560502469226"},"user_tz":300},"id":"Y36K4Eux3h-X","outputId":"3f3aaed9-1cee-4bb6-aba5-f407ca957e16"},"outputs":[{"output_type":"stream","name":"stdout","text":["TensorFlow 1.x selected.\n"]}],"source":["# Use TensorFlow 1.x:\n","%tensorflow_version 1.x"]},{"cell_type":"markdown","metadata":{"id":"cQ-nlTkri4HZ"},"source":["## Link your Google Drive (with your labeled data, or the demo data):\n","\n","### First, place your porject folder into you google drive! \"i.e. move the folder named \"Project-YourName-TheDate\" into google drive."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1148,"status":"ok","timestamp":1646684623922,"user":{"displayName":"Raima Sen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPtYpIHHudDQRAtyYm8EgdjpP9gnylWwSSjx-q=s64","userId":"00263744560502469226"},"user_tz":300},"id":"KS4Q4UkR9rgG","outputId":"bc63b7e6-42f0-4468-aca8-3889febcc796"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["#Now, let's link to your GoogleDrive. Run this cell and follow the authorization instructions:\n","#(We recommend putting a copy of the github repo in your google drive if you are using the demo \"examples\")\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"Frnj1RVDyEqs"},"source":["YOU WILL NEED TO EDIT THE PROJECT PATH **in the config.yaml file** TO BE SET TO YOUR GOOGLE DRIVE LINK!\n","\n","Typically, this will be: /content/drive/My Drive/yourProjectFolderName\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1646684623922,"user":{"displayName":"Raima Sen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPtYpIHHudDQRAtyYm8EgdjpP9gnylWwSSjx-q=s64","userId":"00263744560502469226"},"user_tz":300},"id":"vhENAlQnFENJ","outputId":"f34aef1b-43b5-458c-fd91-e31b99eed09a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/content/drive/My Drive/DLC/videos/']"]},"metadata":{},"execution_count":4}],"source":["#Setup your project variables:\n","# PLEASE EDIT THESE:\n","  \n","ProjectFolderName = 'DLC'\n","VideoType = 'mp4' \n","\n","#don't edit these:\n","videofile_path = ['/content/drive/My Drive/'+ProjectFolderName+'/videos/'] #Enter the list of videos or folder to analyze.\n","videofile_path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sXufoX6INe6w"},"outputs":[],"source":["#GUIs don't work on the cloud, so label your data locally on your computer! This will suppress the GUI support\n","import os\n","os.environ[\"DLClight\"]=\"True\""]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3777,"status":"ok","timestamp":1646684627692,"user":{"displayName":"Raima Sen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPtYpIHHudDQRAtyYm8EgdjpP9gnylWwSSjx-q=s64","userId":"00263744560502469226"},"user_tz":300},"id":"3K9Ndy1beyfG","outputId":"68fa490b-01c2-4122-f1cd-33dc44d58d70"},"outputs":[{"output_type":"stream","name":"stdout","text":["DLC loaded in light mode; you cannot use any GUI (labeling, relabeling and standalone GUI)\n"]}],"source":["import deeplabcut"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1646684627693,"user":{"displayName":"Raima Sen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPtYpIHHudDQRAtyYm8EgdjpP9gnylWwSSjx-q=s64","userId":"00263744560502469226"},"user_tz":300},"id":"o4orkg9QTHKK","outputId":"22dab064-6768-47ee-ee8e-12b5333eecd5"},"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'2.2.0.6'"]},"metadata":{},"execution_count":7}],"source":["deeplabcut.__version__"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":154,"status":"ok","timestamp":1646684627822,"user":{"displayName":"Raima Sen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPtYpIHHudDQRAtyYm8EgdjpP9gnylWwSSjx-q=s64","userId":"00263744560502469226"},"user_tz":300},"id":"Z7ZlDr3wV4D1","outputId":"50bf1695-f5e7-413e-8e6a-2a45cb2bb053"},"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/DLC/config.yaml'"]},"metadata":{},"execution_count":8}],"source":["#This creates a path variable that links to your google drive copy\n","#No need to edit this, as you set it up before: \n","path_config_file = '/content/drive/My Drive/'+ProjectFolderName+'/config.yaml'\n","path_config_file"]},{"cell_type":"markdown","metadata":{"id":"xNi9s1dboEJN"},"source":["## Create a training dataset:\n","### You must do this step inside of Colab:\n","After running this script the training dataset is created and saved in the project directory under the subdirectory **'training-datasets'**\n","\n","This function also creates new subdirectories under **dlc-models** and appends the project config.yaml file with the correct path to the training and testing pose configuration file. These files hold the parameters for training the network. Such an example file is provided with the toolbox and named as **pose_cfg.yaml**.\n","\n","Now it is the time to start training the network!"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":694,"status":"ok","timestamp":1646684628511,"user":{"displayName":"Raima Sen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPtYpIHHudDQRAtyYm8EgdjpP9gnylWwSSjx-q=s64","userId":"00263744560502469226"},"user_tz":300},"id":"eMeUwgxPoEJP","outputId":"cba9d8d2-6475-442b-fc4a-bb000f00691b","scrolled":true},"outputs":[{"output_type":"stream","name":"stdout","text":["The training dataset is successfully created. Use the function 'train_network' to start training. Happy training!\n"]},{"output_type":"execute_result","data":{"text/plain":["[(0.95,\n","  1,\n","  (array([ 3,  9,  4,  5, 17, 16, 14,  7, 13, 10,  6, 12, 11,  1,  0,  8, 15,\n","          18]), array([2])))]"]},"metadata":{},"execution_count":9}],"source":["# Note: if you are using the demo data (i.e. examples/Reaching-Mackenzie-2018-08-30/), first delete the folder called dlc-models! \n","#Then, run this cell. There are many more functions you can set here, including which netowkr to use!\n","#check the docstring for full options you can do!\n","deeplabcut.create_training_dataset(path_config_file, net_type='resnet_50', augmenter_type='imgaug')"]},{"cell_type":"markdown","metadata":{"id":"c4FczXGDoEJU"},"source":["## Start training:\n","This function trains the network for a specific shuffle of the training dataset. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_pOvDq_2oEJW","outputId":"0e2ca93b-dd8b-4b7f-99b9-90aacb0e2050"},"outputs":[{"output_type":"stream","name":"stderr","text":["Config:\n","{'all_joints': [[0],\n","                [1],\n","                [2],\n","                [3],\n","                [4],\n","                [5],\n","                [6],\n","                [7],\n","                [8],\n","                [9],\n","                [10],\n","                [11],\n","                [12],\n","                [13],\n","                [14],\n","                [15]],\n"," 'all_joints_names': ['p_left_wrist',\n","                      'p_right_wrist',\n","                      'p_left_elbow',\n","                      'p_right_elbow',\n","                      'p_left_shoulder',\n","                      'p_right_shoulder',\n","                      'p_neck',\n","                      'p_waist',\n","                      't_left_wrist',\n","                      't_right_wrist',\n","                      't_left_elbow',\n","                      't_right_elbow',\n","                      't_left_shoulder',\n","                      't_right_shoulder',\n","                      't_neck',\n","                      't_waist'],\n"," 'alpha_r': 0.02,\n"," 'apply_prob': 0.5,\n"," 'batch_size': 1,\n"," 'clahe': True,\n"," 'claheratio': 0.1,\n"," 'crop_pad': 0,\n"," 'crop_sampling': 'hybrid',\n"," 'crop_size': [400, 400],\n"," 'cropratio': 0.4,\n"," 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_car_wash_small_markerJan11/car_wash_small_marker_raimasen95shuffle1.mat',\n"," 'dataset_type': 'imgaug',\n"," 'decay_steps': 30000,\n"," 'deterministic': False,\n"," 'display_iters': 1000,\n"," 'edge': False,\n"," 'emboss': {'alpha': [0.0, 1.0], 'embossratio': 0.1, 'strength': [0.5, 1.5]},\n"," 'fg_fraction': 0.25,\n"," 'global_scale': 0.8,\n"," 'histeq': True,\n"," 'histeqratio': 0.1,\n"," 'init_weights': '/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt',\n"," 'intermediate_supervision': False,\n"," 'intermediate_supervision_layer': 12,\n"," 'location_refinement': True,\n"," 'locref_huber_loss': True,\n"," 'locref_loss_weight': 0.05,\n"," 'locref_stdev': 7.2801,\n"," 'log_dir': 'log',\n"," 'lr_init': 0.0005,\n"," 'max_input_size': 1500,\n"," 'max_shift': 0.4,\n"," 'mean_pixel': [123.68, 116.779, 103.939],\n"," 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_car_wash_small_markerJan11/Documentation_data-car_wash_small_marker_95shuffle1.pickle',\n"," 'min_input_size': 64,\n"," 'mirror': False,\n"," 'multi_stage': False,\n"," 'multi_step': [[0.005, 10000],\n","                [0.02, 430000],\n","                [0.002, 730000],\n","                [0.001, 1030000]],\n"," 'net_type': 'resnet_50',\n"," 'num_joints': 16,\n"," 'optimizer': 'sgd',\n"," 'pairwise_huber_loss': False,\n"," 'pairwise_predict': False,\n"," 'partaffinityfield_predict': False,\n"," 'pos_dist_thresh': 17,\n"," 'pre_resize': [],\n"," 'project_path': '/content/drive/My Drive/DLC',\n"," 'regularize': False,\n"," 'rotation': 25,\n"," 'rotratio': 0.4,\n"," 'save_iters': 50000,\n"," 'scale_jitter_lo': 0.5,\n"," 'scale_jitter_up': 1.25,\n"," 'scoremap_dir': 'test',\n"," 'sharpen': False,\n"," 'sharpenratio': 0.3,\n"," 'shuffle': True,\n"," 'snapshot_prefix': '/content/drive/My '\n","                    'Drive/DLC/dlc-models/iteration-0/car_wash_small_markerJan11-trainset95shuffle1/train/snapshot',\n"," 'stride': 8.0,\n"," 'weigh_negatives': False,\n"," 'weigh_only_present_joints': False,\n"," 'weigh_part_predictions': False,\n"," 'weight_decay': 0.0001}\n"]},{"output_type":"stream","name":"stdout","text":["Selecting single-animal trainer\n","Batch Size is 1\n","Loading ImageNet-pretrained resnet_50\n","Display_iters overwritten as 10\n","Save_iters overwritten as 500\n","Training parameter:\n","{'stride': 8.0, 'weigh_part_predictions': False, 'weigh_negatives': False, 'fg_fraction': 0.25, 'mean_pixel': [123.68, 116.779, 103.939], 'shuffle': True, 'snapshot_prefix': '/content/drive/My Drive/DLC/dlc-models/iteration-0/car_wash_small_markerJan11-trainset95shuffle1/train/snapshot', 'log_dir': 'log', 'global_scale': 0.8, 'location_refinement': True, 'locref_stdev': 7.2801, 'locref_loss_weight': 0.05, 'locref_huber_loss': True, 'optimizer': 'sgd', 'intermediate_supervision': False, 'intermediate_supervision_layer': 12, 'regularize': False, 'weight_decay': 0.0001, 'crop_pad': 0, 'scoremap_dir': 'test', 'batch_size': 1, 'dataset_type': 'imgaug', 'deterministic': False, 'mirror': False, 'pairwise_huber_loss': False, 'weigh_only_present_joints': False, 'partaffinityfield_predict': False, 'pairwise_predict': False, 'all_joints': [[0], [1], [2], [3], [4], [5], [6], [7], [8], [9], [10], [11], [12], [13], [14], [15]], 'all_joints_names': ['p_left_wrist', 'p_right_wrist', 'p_left_elbow', 'p_right_elbow', 'p_left_shoulder', 'p_right_shoulder', 'p_neck', 'p_waist', 't_left_wrist', 't_right_wrist', 't_left_elbow', 't_right_elbow', 't_left_shoulder', 't_right_shoulder', 't_neck', 't_waist'], 'alpha_r': 0.02, 'apply_prob': 0.5, 'clahe': True, 'claheratio': 0.1, 'crop_sampling': 'hybrid', 'crop_size': [400, 400], 'cropratio': 0.4, 'dataset': 'training-datasets/iteration-0/UnaugmentedDataSet_car_wash_small_markerJan11/car_wash_small_marker_raimasen95shuffle1.mat', 'decay_steps': 30000, 'display_iters': 1000, 'edge': False, 'emboss': {'alpha': [0.0, 1.0], 'embossratio': 0.1, 'strength': [0.5, 1.5]}, 'histeq': True, 'histeqratio': 0.1, 'init_weights': '/usr/local/lib/python3.7/dist-packages/deeplabcut/pose_estimation_tensorflow/models/pretrained/resnet_v1_50.ckpt', 'lr_init': 0.0005, 'max_input_size': 1500, 'max_shift': 0.4, 'metadataset': 'training-datasets/iteration-0/UnaugmentedDataSet_car_wash_small_markerJan11/Documentation_data-car_wash_small_marker_95shuffle1.pickle', 'min_input_size': 64, 'multi_stage': False, 'multi_step': [[0.005, 10000], [0.02, 430000], [0.002, 730000], [0.001, 1030000]], 'net_type': 'resnet_50', 'num_joints': 16, 'pos_dist_thresh': 17, 'pre_resize': [], 'project_path': '/content/drive/My Drive/DLC', 'rotation': 25, 'rotratio': 0.4, 'save_iters': 50000, 'scale_jitter_lo': 0.5, 'scale_jitter_up': 1.25, 'sharpen': False, 'sharpenratio': 0.3, 'covering': True, 'elastic_transform': True, 'motion_blur': True, 'motion_blur_params': {'k': 7, 'angle': (-90, 90)}}\n","Starting training....\n"]},{"output_type":"stream","name":"stderr","text":["iteration: 10 loss: 0.5321 lr: 0.005\n","iteration: 20 loss: 0.0677 lr: 0.005\n","iteration: 30 loss: 0.0454 lr: 0.005\n","iteration: 40 loss: 0.0377 lr: 0.005\n","iteration: 50 loss: 0.0336 lr: 0.005\n","iteration: 60 loss: 0.0336 lr: 0.005\n","iteration: 70 loss: 0.0322 lr: 0.005\n","iteration: 80 loss: 0.0352 lr: 0.005\n","iteration: 90 loss: 0.0357 lr: 0.005\n","iteration: 100 loss: 0.0326 lr: 0.005\n","iteration: 110 loss: 0.0349 lr: 0.005\n","iteration: 120 loss: 0.0338 lr: 0.005\n","iteration: 130 loss: 0.0314 lr: 0.005\n","iteration: 140 loss: 0.0325 lr: 0.005\n","iteration: 150 loss: 0.0302 lr: 0.005\n","iteration: 160 loss: 0.0308 lr: 0.005\n","iteration: 170 loss: 0.0329 lr: 0.005\n","iteration: 180 loss: 0.0294 lr: 0.005\n","iteration: 190 loss: 0.0321 lr: 0.005\n","iteration: 200 loss: 0.0302 lr: 0.005\n","iteration: 210 loss: 0.0292 lr: 0.005\n","iteration: 220 loss: 0.0263 lr: 0.005\n","iteration: 230 loss: 0.0301 lr: 0.005\n","iteration: 240 loss: 0.0304 lr: 0.005\n","iteration: 250 loss: 0.0318 lr: 0.005\n","iteration: 260 loss: 0.0329 lr: 0.005\n","iteration: 270 loss: 0.0274 lr: 0.005\n","iteration: 280 loss: 0.0308 lr: 0.005\n","iteration: 290 loss: 0.0302 lr: 0.005\n","iteration: 300 loss: 0.0301 lr: 0.005\n","iteration: 310 loss: 0.0331 lr: 0.005\n","iteration: 320 loss: 0.0333 lr: 0.005\n","iteration: 330 loss: 0.0309 lr: 0.005\n","iteration: 340 loss: 0.0291 lr: 0.005\n","iteration: 350 loss: 0.0304 lr: 0.005\n","iteration: 360 loss: 0.0297 lr: 0.005\n","iteration: 370 loss: 0.0279 lr: 0.005\n","iteration: 380 loss: 0.0283 lr: 0.005\n","iteration: 390 loss: 0.0270 lr: 0.005\n","iteration: 400 loss: 0.0285 lr: 0.005\n","iteration: 410 loss: 0.0311 lr: 0.005\n","iteration: 420 loss: 0.0286 lr: 0.005\n","iteration: 430 loss: 0.0284 lr: 0.005\n","iteration: 440 loss: 0.0285 lr: 0.005\n","iteration: 450 loss: 0.0285 lr: 0.005\n","iteration: 460 loss: 0.0302 lr: 0.005\n","iteration: 470 loss: 0.0266 lr: 0.005\n","iteration: 480 loss: 0.0268 lr: 0.005\n","iteration: 490 loss: 0.0258 lr: 0.005\n","iteration: 500 loss: 0.0251 lr: 0.005\n","iteration: 510 loss: 0.0306 lr: 0.005\n","iteration: 520 loss: 0.0289 lr: 0.005\n","iteration: 530 loss: 0.0278 lr: 0.005\n","iteration: 540 loss: 0.0277 lr: 0.005\n","iteration: 550 loss: 0.0272 lr: 0.005\n","iteration: 560 loss: 0.0293 lr: 0.005\n","iteration: 570 loss: 0.0290 lr: 0.005\n","iteration: 580 loss: 0.0239 lr: 0.005\n","iteration: 590 loss: 0.0296 lr: 0.005\n","iteration: 600 loss: 0.0287 lr: 0.005\n","iteration: 610 loss: 0.0265 lr: 0.005\n","iteration: 620 loss: 0.0258 lr: 0.005\n","iteration: 630 loss: 0.0237 lr: 0.005\n","iteration: 640 loss: 0.0249 lr: 0.005\n","iteration: 650 loss: 0.0257 lr: 0.005\n","iteration: 660 loss: 0.0296 lr: 0.005\n","iteration: 670 loss: 0.0244 lr: 0.005\n","iteration: 680 loss: 0.0259 lr: 0.005\n","iteration: 690 loss: 0.0239 lr: 0.005\n","iteration: 700 loss: 0.0214 lr: 0.005\n","iteration: 710 loss: 0.0258 lr: 0.005\n","iteration: 720 loss: 0.0228 lr: 0.005\n","iteration: 730 loss: 0.0229 lr: 0.005\n","iteration: 740 loss: 0.0223 lr: 0.005\n","iteration: 750 loss: 0.0234 lr: 0.005\n","iteration: 760 loss: 0.0235 lr: 0.005\n","iteration: 770 loss: 0.0263 lr: 0.005\n","iteration: 780 loss: 0.0229 lr: 0.005\n","iteration: 790 loss: 0.0226 lr: 0.005\n","iteration: 800 loss: 0.0222 lr: 0.005\n","iteration: 810 loss: 0.0247 lr: 0.005\n","iteration: 820 loss: 0.0251 lr: 0.005\n","iteration: 830 loss: 0.0227 lr: 0.005\n","iteration: 840 loss: 0.0252 lr: 0.005\n","iteration: 850 loss: 0.0222 lr: 0.005\n","iteration: 860 loss: 0.0245 lr: 0.005\n","iteration: 870 loss: 0.0198 lr: 0.005\n","iteration: 880 loss: 0.0230 lr: 0.005\n","iteration: 890 loss: 0.0226 lr: 0.005\n","iteration: 900 loss: 0.0212 lr: 0.005\n","iteration: 910 loss: 0.0247 lr: 0.005\n","iteration: 920 loss: 0.0231 lr: 0.005\n","iteration: 930 loss: 0.0214 lr: 0.005\n","iteration: 940 loss: 0.0222 lr: 0.005\n","iteration: 950 loss: 0.0236 lr: 0.005\n","iteration: 960 loss: 0.0194 lr: 0.005\n","iteration: 970 loss: 0.0219 lr: 0.005\n","iteration: 980 loss: 0.0236 lr: 0.005\n","iteration: 990 loss: 0.0197 lr: 0.005\n","iteration: 1000 loss: 0.0191 lr: 0.005\n","iteration: 1010 loss: 0.0200 lr: 0.005\n","iteration: 1020 loss: 0.0225 lr: 0.005\n","iteration: 1030 loss: 0.0204 lr: 0.005\n","iteration: 1040 loss: 0.0231 lr: 0.005\n","iteration: 1050 loss: 0.0206 lr: 0.005\n","iteration: 1060 loss: 0.0208 lr: 0.005\n","iteration: 1070 loss: 0.0219 lr: 0.005\n","iteration: 1080 loss: 0.0225 lr: 0.005\n","iteration: 1090 loss: 0.0196 lr: 0.005\n","iteration: 1100 loss: 0.0231 lr: 0.005\n","iteration: 1110 loss: 0.0233 lr: 0.005\n","iteration: 1120 loss: 0.0198 lr: 0.005\n","iteration: 1130 loss: 0.0205 lr: 0.005\n","iteration: 1140 loss: 0.0198 lr: 0.005\n","iteration: 1150 loss: 0.0213 lr: 0.005\n","iteration: 1160 loss: 0.0198 lr: 0.005\n","iteration: 1170 loss: 0.0202 lr: 0.005\n","iteration: 1180 loss: 0.0212 lr: 0.005\n","iteration: 1190 loss: 0.0187 lr: 0.005\n","iteration: 1200 loss: 0.0235 lr: 0.005\n","iteration: 1210 loss: 0.0214 lr: 0.005\n","iteration: 1220 loss: 0.0199 lr: 0.005\n","iteration: 1230 loss: 0.0158 lr: 0.005\n","iteration: 1240 loss: 0.0185 lr: 0.005\n","iteration: 1250 loss: 0.0181 lr: 0.005\n","iteration: 1260 loss: 0.0195 lr: 0.005\n","iteration: 1270 loss: 0.0179 lr: 0.005\n","iteration: 1280 loss: 0.0186 lr: 0.005\n","iteration: 1290 loss: 0.0193 lr: 0.005\n","iteration: 1300 loss: 0.0191 lr: 0.005\n","iteration: 1310 loss: 0.0190 lr: 0.005\n","iteration: 1320 loss: 0.0192 lr: 0.005\n","iteration: 1330 loss: 0.0199 lr: 0.005\n","iteration: 1340 loss: 0.0200 lr: 0.005\n","iteration: 1350 loss: 0.0184 lr: 0.005\n","iteration: 1360 loss: 0.0183 lr: 0.005\n","iteration: 1370 loss: 0.0189 lr: 0.005\n","iteration: 1380 loss: 0.0193 lr: 0.005\n","iteration: 1390 loss: 0.0185 lr: 0.005\n","iteration: 1400 loss: 0.0198 lr: 0.005\n","iteration: 1410 loss: 0.0186 lr: 0.005\n","iteration: 1420 loss: 0.0181 lr: 0.005\n","iteration: 1430 loss: 0.0181 lr: 0.005\n","iteration: 1440 loss: 0.0194 lr: 0.005\n","iteration: 1450 loss: 0.0185 lr: 0.005\n","iteration: 1460 loss: 0.0173 lr: 0.005\n","iteration: 1470 loss: 0.0188 lr: 0.005\n","iteration: 1480 loss: 0.0165 lr: 0.005\n","iteration: 1490 loss: 0.0187 lr: 0.005\n","iteration: 1500 loss: 0.0198 lr: 0.005\n","iteration: 1510 loss: 0.0179 lr: 0.005\n","iteration: 1520 loss: 0.0203 lr: 0.005\n","iteration: 1530 loss: 0.0202 lr: 0.005\n","iteration: 1540 loss: 0.0198 lr: 0.005\n","iteration: 1550 loss: 0.0185 lr: 0.005\n","iteration: 1560 loss: 0.0190 lr: 0.005\n","iteration: 1570 loss: 0.0181 lr: 0.005\n","iteration: 1580 loss: 0.0201 lr: 0.005\n","iteration: 1590 loss: 0.0159 lr: 0.005\n","iteration: 1600 loss: 0.0188 lr: 0.005\n","iteration: 1610 loss: 0.0177 lr: 0.005\n","iteration: 1620 loss: 0.0174 lr: 0.005\n","iteration: 1630 loss: 0.0174 lr: 0.005\n","iteration: 1640 loss: 0.0188 lr: 0.005\n","iteration: 1650 loss: 0.0176 lr: 0.005\n","iteration: 1660 loss: 0.0170 lr: 0.005\n","iteration: 1670 loss: 0.0169 lr: 0.005\n","iteration: 1680 loss: 0.0171 lr: 0.005\n","iteration: 1690 loss: 0.0166 lr: 0.005\n","iteration: 1700 loss: 0.0173 lr: 0.005\n","iteration: 1710 loss: 0.0191 lr: 0.005\n","iteration: 1720 loss: 0.0174 lr: 0.005\n","iteration: 1730 loss: 0.0156 lr: 0.005\n","iteration: 1740 loss: 0.0155 lr: 0.005\n","iteration: 1750 loss: 0.0160 lr: 0.005\n","iteration: 1760 loss: 0.0167 lr: 0.005\n","iteration: 1770 loss: 0.0164 lr: 0.005\n","iteration: 1780 loss: 0.0151 lr: 0.005\n","iteration: 1790 loss: 0.0169 lr: 0.005\n","iteration: 1800 loss: 0.0186 lr: 0.005\n","iteration: 1810 loss: 0.0143 lr: 0.005\n","iteration: 1820 loss: 0.0171 lr: 0.005\n","iteration: 1830 loss: 0.0155 lr: 0.005\n","iteration: 1840 loss: 0.0176 lr: 0.005\n","iteration: 1850 loss: 0.0187 lr: 0.005\n","iteration: 1860 loss: 0.0151 lr: 0.005\n","iteration: 1870 loss: 0.0160 lr: 0.005\n","iteration: 1880 loss: 0.0164 lr: 0.005\n","iteration: 1890 loss: 0.0177 lr: 0.005\n","iteration: 1900 loss: 0.0181 lr: 0.005\n","iteration: 1910 loss: 0.0170 lr: 0.005\n","iteration: 1920 loss: 0.0169 lr: 0.005\n","iteration: 1930 loss: 0.0171 lr: 0.005\n","iteration: 1940 loss: 0.0173 lr: 0.005\n","iteration: 1950 loss: 0.0161 lr: 0.005\n","iteration: 1960 loss: 0.0174 lr: 0.005\n","iteration: 1970 loss: 0.0162 lr: 0.005\n","iteration: 1980 loss: 0.0161 lr: 0.005\n","iteration: 1990 loss: 0.0148 lr: 0.005\n","iteration: 2000 loss: 0.0153 lr: 0.005\n","iteration: 2010 loss: 0.0167 lr: 0.005\n","iteration: 2020 loss: 0.0161 lr: 0.005\n","iteration: 2030 loss: 0.0142 lr: 0.005\n","iteration: 2040 loss: 0.0176 lr: 0.005\n","iteration: 2050 loss: 0.0141 lr: 0.005\n","iteration: 2060 loss: 0.0157 lr: 0.005\n","iteration: 2070 loss: 0.0167 lr: 0.005\n","iteration: 2080 loss: 0.0157 lr: 0.005\n","iteration: 2090 loss: 0.0162 lr: 0.005\n","iteration: 2100 loss: 0.0143 lr: 0.005\n","iteration: 2110 loss: 0.0157 lr: 0.005\n","iteration: 2120 loss: 0.0151 lr: 0.005\n","iteration: 2130 loss: 0.0156 lr: 0.005\n","iteration: 2140 loss: 0.0127 lr: 0.005\n","iteration: 2150 loss: 0.0168 lr: 0.005\n","iteration: 2160 loss: 0.0156 lr: 0.005\n","iteration: 2170 loss: 0.0152 lr: 0.005\n","iteration: 2180 loss: 0.0159 lr: 0.005\n","iteration: 2190 loss: 0.0153 lr: 0.005\n","iteration: 2200 loss: 0.0166 lr: 0.005\n","iteration: 2210 loss: 0.0152 lr: 0.005\n","iteration: 2220 loss: 0.0133 lr: 0.005\n","iteration: 2230 loss: 0.0144 lr: 0.005\n","iteration: 2240 loss: 0.0159 lr: 0.005\n","iteration: 2250 loss: 0.0139 lr: 0.005\n","iteration: 2260 loss: 0.0155 lr: 0.005\n","iteration: 2270 loss: 0.0144 lr: 0.005\n","iteration: 2280 loss: 0.0141 lr: 0.005\n","iteration: 2290 loss: 0.0158 lr: 0.005\n","iteration: 2300 loss: 0.0136 lr: 0.005\n","iteration: 2310 loss: 0.0157 lr: 0.005\n","iteration: 2320 loss: 0.0157 lr: 0.005\n","iteration: 2330 loss: 0.0171 lr: 0.005\n","iteration: 2340 loss: 0.0161 lr: 0.005\n","iteration: 2350 loss: 0.0148 lr: 0.005\n","iteration: 2360 loss: 0.0143 lr: 0.005\n","iteration: 2370 loss: 0.0142 lr: 0.005\n","iteration: 2380 loss: 0.0139 lr: 0.005\n","iteration: 2390 loss: 0.0140 lr: 0.005\n","iteration: 2400 loss: 0.0140 lr: 0.005\n","iteration: 2410 loss: 0.0137 lr: 0.005\n","iteration: 2420 loss: 0.0171 lr: 0.005\n","iteration: 2430 loss: 0.0153 lr: 0.005\n","iteration: 2440 loss: 0.0159 lr: 0.005\n","iteration: 2450 loss: 0.0137 lr: 0.005\n","iteration: 2460 loss: 0.0131 lr: 0.005\n","iteration: 2470 loss: 0.0143 lr: 0.005\n","iteration: 2480 loss: 0.0154 lr: 0.005\n","iteration: 2490 loss: 0.0149 lr: 0.005\n","iteration: 2500 loss: 0.0161 lr: 0.005\n","iteration: 2510 loss: 0.0155 lr: 0.005\n","iteration: 2520 loss: 0.0129 lr: 0.005\n","iteration: 2530 loss: 0.0150 lr: 0.005\n","iteration: 2540 loss: 0.0134 lr: 0.005\n","iteration: 2550 loss: 0.0151 lr: 0.005\n","iteration: 2560 loss: 0.0154 lr: 0.005\n","iteration: 2570 loss: 0.0149 lr: 0.005\n","iteration: 2580 loss: 0.0168 lr: 0.005\n","iteration: 2590 loss: 0.0162 lr: 0.005\n","iteration: 2600 loss: 0.0133 lr: 0.005\n","iteration: 2610 loss: 0.0148 lr: 0.005\n","iteration: 2620 loss: 0.0140 lr: 0.005\n","iteration: 2630 loss: 0.0124 lr: 0.005\n","iteration: 2640 loss: 0.0140 lr: 0.005\n","iteration: 2650 loss: 0.0137 lr: 0.005\n","iteration: 2660 loss: 0.0151 lr: 0.005\n","iteration: 2670 loss: 0.0148 lr: 0.005\n","iteration: 2680 loss: 0.0128 lr: 0.005\n","iteration: 2690 loss: 0.0129 lr: 0.005\n","iteration: 2700 loss: 0.0162 lr: 0.005\n","iteration: 2710 loss: 0.0154 lr: 0.005\n","iteration: 2720 loss: 0.0138 lr: 0.005\n","iteration: 2730 loss: 0.0153 lr: 0.005\n","iteration: 2740 loss: 0.0134 lr: 0.005\n","iteration: 2750 loss: 0.0135 lr: 0.005\n","iteration: 2760 loss: 0.0129 lr: 0.005\n","iteration: 2770 loss: 0.0150 lr: 0.005\n","iteration: 2780 loss: 0.0142 lr: 0.005\n","iteration: 2790 loss: 0.0130 lr: 0.005\n","iteration: 2800 loss: 0.0144 lr: 0.005\n","iteration: 2810 loss: 0.0143 lr: 0.005\n","iteration: 2820 loss: 0.0140 lr: 0.005\n","iteration: 2830 loss: 0.0148 lr: 0.005\n","iteration: 2840 loss: 0.0137 lr: 0.005\n","iteration: 2850 loss: 0.0140 lr: 0.005\n","iteration: 2860 loss: 0.0147 lr: 0.005\n","iteration: 2870 loss: 0.0153 lr: 0.005\n","iteration: 2880 loss: 0.0127 lr: 0.005\n","iteration: 2890 loss: 0.0125 lr: 0.005\n","iteration: 2900 loss: 0.0126 lr: 0.005\n","iteration: 2910 loss: 0.0153 lr: 0.005\n","iteration: 2920 loss: 0.0135 lr: 0.005\n","iteration: 2930 loss: 0.0147 lr: 0.005\n","iteration: 2940 loss: 0.0139 lr: 0.005\n","iteration: 2950 loss: 0.0148 lr: 0.005\n","iteration: 2960 loss: 0.0131 lr: 0.005\n","iteration: 2970 loss: 0.0144 lr: 0.005\n","iteration: 2980 loss: 0.0125 lr: 0.005\n","iteration: 2990 loss: 0.0131 lr: 0.005\n","iteration: 3000 loss: 0.0139 lr: 0.005\n","iteration: 3010 loss: 0.0155 lr: 0.005\n","iteration: 3020 loss: 0.0141 lr: 0.005\n","iteration: 3030 loss: 0.0145 lr: 0.005\n","iteration: 3040 loss: 0.0118 lr: 0.005\n","iteration: 3050 loss: 0.0117 lr: 0.005\n","iteration: 3060 loss: 0.0120 lr: 0.005\n","iteration: 3070 loss: 0.0130 lr: 0.005\n","iteration: 3080 loss: 0.0141 lr: 0.005\n","iteration: 3090 loss: 0.0142 lr: 0.005\n","iteration: 3100 loss: 0.0135 lr: 0.005\n","iteration: 3110 loss: 0.0122 lr: 0.005\n","iteration: 3120 loss: 0.0129 lr: 0.005\n","iteration: 3130 loss: 0.0132 lr: 0.005\n","iteration: 3140 loss: 0.0135 lr: 0.005\n","iteration: 3150 loss: 0.0138 lr: 0.005\n","iteration: 3160 loss: 0.0135 lr: 0.005\n","iteration: 3170 loss: 0.0141 lr: 0.005\n","iteration: 3180 loss: 0.0121 lr: 0.005\n","iteration: 3190 loss: 0.0126 lr: 0.005\n","iteration: 3200 loss: 0.0125 lr: 0.005\n","iteration: 3210 loss: 0.0131 lr: 0.005\n","iteration: 3220 loss: 0.0128 lr: 0.005\n","iteration: 3230 loss: 0.0127 lr: 0.005\n","iteration: 3240 loss: 0.0120 lr: 0.005\n","iteration: 3250 loss: 0.0120 lr: 0.005\n","iteration: 3260 loss: 0.0118 lr: 0.005\n","iteration: 3270 loss: 0.0124 lr: 0.005\n","iteration: 3280 loss: 0.0135 lr: 0.005\n","iteration: 3290 loss: 0.0125 lr: 0.005\n","iteration: 3300 loss: 0.0129 lr: 0.005\n","iteration: 3310 loss: 0.0123 lr: 0.005\n","iteration: 3320 loss: 0.0125 lr: 0.005\n","iteration: 3330 loss: 0.0114 lr: 0.005\n","iteration: 3340 loss: 0.0145 lr: 0.005\n","iteration: 3350 loss: 0.0129 lr: 0.005\n","iteration: 3360 loss: 0.0135 lr: 0.005\n","iteration: 3370 loss: 0.0125 lr: 0.005\n","iteration: 3380 loss: 0.0114 lr: 0.005\n","iteration: 3390 loss: 0.0127 lr: 0.005\n","iteration: 3400 loss: 0.0120 lr: 0.005\n","iteration: 3410 loss: 0.0121 lr: 0.005\n","iteration: 3420 loss: 0.0124 lr: 0.005\n","iteration: 3430 loss: 0.0139 lr: 0.005\n","iteration: 3440 loss: 0.0126 lr: 0.005\n","iteration: 3450 loss: 0.0137 lr: 0.005\n","iteration: 3460 loss: 0.0154 lr: 0.005\n","iteration: 3470 loss: 0.0124 lr: 0.005\n","iteration: 3480 loss: 0.0129 lr: 0.005\n","iteration: 3490 loss: 0.0121 lr: 0.005\n","iteration: 3500 loss: 0.0128 lr: 0.005\n","iteration: 3510 loss: 0.0123 lr: 0.005\n","iteration: 3520 loss: 0.0115 lr: 0.005\n","iteration: 3530 loss: 0.0136 lr: 0.005\n","iteration: 3540 loss: 0.0131 lr: 0.005\n","iteration: 3550 loss: 0.0135 lr: 0.005\n","iteration: 3560 loss: 0.0145 lr: 0.005\n","iteration: 3570 loss: 0.0125 lr: 0.005\n","iteration: 3580 loss: 0.0119 lr: 0.005\n","iteration: 3590 loss: 0.0136 lr: 0.005\n","iteration: 3600 loss: 0.0113 lr: 0.005\n","iteration: 3610 loss: 0.0110 lr: 0.005\n","iteration: 3620 loss: 0.0128 lr: 0.005\n","iteration: 3630 loss: 0.0117 lr: 0.005\n","iteration: 3640 loss: 0.0117 lr: 0.005\n","iteration: 3650 loss: 0.0112 lr: 0.005\n","iteration: 3660 loss: 0.0114 lr: 0.005\n","iteration: 3670 loss: 0.0137 lr: 0.005\n","iteration: 3680 loss: 0.0122 lr: 0.005\n","iteration: 3690 loss: 0.0132 lr: 0.005\n","iteration: 3700 loss: 0.0125 lr: 0.005\n","iteration: 3710 loss: 0.0112 lr: 0.005\n","iteration: 3720 loss: 0.0121 lr: 0.005\n","iteration: 3730 loss: 0.0129 lr: 0.005\n","iteration: 3740 loss: 0.0109 lr: 0.005\n","iteration: 3750 loss: 0.0113 lr: 0.005\n","iteration: 3760 loss: 0.0127 lr: 0.005\n","iteration: 3770 loss: 0.0106 lr: 0.005\n","iteration: 3780 loss: 0.0141 lr: 0.005\n","iteration: 3790 loss: 0.0130 lr: 0.005\n","iteration: 3800 loss: 0.0122 lr: 0.005\n","iteration: 3810 loss: 0.0134 lr: 0.005\n","iteration: 3820 loss: 0.0127 lr: 0.005\n","iteration: 3830 loss: 0.0116 lr: 0.005\n","iteration: 3840 loss: 0.0131 lr: 0.005\n","iteration: 3850 loss: 0.0115 lr: 0.005\n","iteration: 3860 loss: 0.0126 lr: 0.005\n","iteration: 3870 loss: 0.0118 lr: 0.005\n","iteration: 3880 loss: 0.0121 lr: 0.005\n","iteration: 3890 loss: 0.0128 lr: 0.005\n","iteration: 3900 loss: 0.0111 lr: 0.005\n","iteration: 3910 loss: 0.0143 lr: 0.005\n","iteration: 3920 loss: 0.0113 lr: 0.005\n","iteration: 3930 loss: 0.0118 lr: 0.005\n","iteration: 3940 loss: 0.0118 lr: 0.005\n","iteration: 3950 loss: 0.0112 lr: 0.005\n","iteration: 3960 loss: 0.0127 lr: 0.005\n","iteration: 3970 loss: 0.0116 lr: 0.005\n","iteration: 3980 loss: 0.0121 lr: 0.005\n","iteration: 3990 loss: 0.0119 lr: 0.005\n","iteration: 4000 loss: 0.0110 lr: 0.005\n","iteration: 4010 loss: 0.0128 lr: 0.005\n","iteration: 4020 loss: 0.0122 lr: 0.005\n","iteration: 4030 loss: 0.0126 lr: 0.005\n","iteration: 4040 loss: 0.0117 lr: 0.005\n","iteration: 4050 loss: 0.0115 lr: 0.005\n","iteration: 4060 loss: 0.0107 lr: 0.005\n","iteration: 4070 loss: 0.0120 lr: 0.005\n","iteration: 4080 loss: 0.0131 lr: 0.005\n","iteration: 4090 loss: 0.0108 lr: 0.005\n","iteration: 4100 loss: 0.0133 lr: 0.005\n","iteration: 4110 loss: 0.0117 lr: 0.005\n","iteration: 4120 loss: 0.0107 lr: 0.005\n","iteration: 4130 loss: 0.0126 lr: 0.005\n","iteration: 4140 loss: 0.0115 lr: 0.005\n","iteration: 4150 loss: 0.0129 lr: 0.005\n","iteration: 4160 loss: 0.0113 lr: 0.005\n","iteration: 4170 loss: 0.0131 lr: 0.005\n","iteration: 4180 loss: 0.0112 lr: 0.005\n","iteration: 4190 loss: 0.0119 lr: 0.005\n","iteration: 4200 loss: 0.0121 lr: 0.005\n","iteration: 4210 loss: 0.0111 lr: 0.005\n","iteration: 4220 loss: 0.0119 lr: 0.005\n","iteration: 4230 loss: 0.0121 lr: 0.005\n","iteration: 4240 loss: 0.0106 lr: 0.005\n","iteration: 4250 loss: 0.0130 lr: 0.005\n","iteration: 4260 loss: 0.0113 lr: 0.005\n","iteration: 4270 loss: 0.0122 lr: 0.005\n","iteration: 4280 loss: 0.0119 lr: 0.005\n","iteration: 4290 loss: 0.0120 lr: 0.005\n","iteration: 4300 loss: 0.0122 lr: 0.005\n","iteration: 4310 loss: 0.0118 lr: 0.005\n","iteration: 4320 loss: 0.0119 lr: 0.005\n","iteration: 4330 loss: 0.0140 lr: 0.005\n","iteration: 4340 loss: 0.0109 lr: 0.005\n","iteration: 4350 loss: 0.0108 lr: 0.005\n","iteration: 4360 loss: 0.0111 lr: 0.005\n","iteration: 4370 loss: 0.0112 lr: 0.005\n","iteration: 4380 loss: 0.0124 lr: 0.005\n","iteration: 4390 loss: 0.0106 lr: 0.005\n","iteration: 4400 loss: 0.0123 lr: 0.005\n","iteration: 4410 loss: 0.0115 lr: 0.005\n","iteration: 4420 loss: 0.0119 lr: 0.005\n","iteration: 4430 loss: 0.0125 lr: 0.005\n","iteration: 4440 loss: 0.0107 lr: 0.005\n","iteration: 4450 loss: 0.0108 lr: 0.005\n","iteration: 4460 loss: 0.0120 lr: 0.005\n","iteration: 4470 loss: 0.0120 lr: 0.005\n","iteration: 4480 loss: 0.0111 lr: 0.005\n","iteration: 4490 loss: 0.0124 lr: 0.005\n","iteration: 4500 loss: 0.0110 lr: 0.005\n","iteration: 4510 loss: 0.0115 lr: 0.005\n","iteration: 4520 loss: 0.0106 lr: 0.005\n","iteration: 4530 loss: 0.0118 lr: 0.005\n","iteration: 4540 loss: 0.0104 lr: 0.005\n","iteration: 4550 loss: 0.0109 lr: 0.005\n","iteration: 4560 loss: 0.0108 lr: 0.005\n","iteration: 4570 loss: 0.0100 lr: 0.005\n","iteration: 4580 loss: 0.0101 lr: 0.005\n","iteration: 4590 loss: 0.0116 lr: 0.005\n","iteration: 4600 loss: 0.0114 lr: 0.005\n","iteration: 4610 loss: 0.0106 lr: 0.005\n","iteration: 4620 loss: 0.0119 lr: 0.005\n","iteration: 4630 loss: 0.0099 lr: 0.005\n","iteration: 4640 loss: 0.0110 lr: 0.005\n","iteration: 4650 loss: 0.0126 lr: 0.005\n","iteration: 4660 loss: 0.0122 lr: 0.005\n","iteration: 4670 loss: 0.0110 lr: 0.005\n","iteration: 4680 loss: 0.0112 lr: 0.005\n","iteration: 4690 loss: 0.0098 lr: 0.005\n","iteration: 4700 loss: 0.0115 lr: 0.005\n","iteration: 4710 loss: 0.0107 lr: 0.005\n","iteration: 4720 loss: 0.0105 lr: 0.005\n","iteration: 4730 loss: 0.0117 lr: 0.005\n","iteration: 4740 loss: 0.0110 lr: 0.005\n","iteration: 4750 loss: 0.0114 lr: 0.005\n","iteration: 4760 loss: 0.0113 lr: 0.005\n","iteration: 4770 loss: 0.0105 lr: 0.005\n","iteration: 4780 loss: 0.0108 lr: 0.005\n","iteration: 4790 loss: 0.0114 lr: 0.005\n","iteration: 4800 loss: 0.0104 lr: 0.005\n","iteration: 4810 loss: 0.0104 lr: 0.005\n","iteration: 4820 loss: 0.0093 lr: 0.005\n","iteration: 4830 loss: 0.0103 lr: 0.005\n","iteration: 4840 loss: 0.0112 lr: 0.005\n","iteration: 4850 loss: 0.0102 lr: 0.005\n","iteration: 4860 loss: 0.0089 lr: 0.005\n","iteration: 4870 loss: 0.0131 lr: 0.005\n","iteration: 4880 loss: 0.0114 lr: 0.005\n","iteration: 4890 loss: 0.0112 lr: 0.005\n","iteration: 4900 loss: 0.0097 lr: 0.005\n","iteration: 4910 loss: 0.0107 lr: 0.005\n","iteration: 4920 loss: 0.0114 lr: 0.005\n","iteration: 4930 loss: 0.0117 lr: 0.005\n","iteration: 4940 loss: 0.0111 lr: 0.005\n","iteration: 4950 loss: 0.0117 lr: 0.005\n","iteration: 4960 loss: 0.0128 lr: 0.005\n","iteration: 4970 loss: 0.0109 lr: 0.005\n","iteration: 4980 loss: 0.0115 lr: 0.005\n","iteration: 4990 loss: 0.0105 lr: 0.005\n","iteration: 5000 loss: 0.0102 lr: 0.005\n","iteration: 5010 loss: 0.0100 lr: 0.005\n","iteration: 5020 loss: 0.0101 lr: 0.005\n","iteration: 5030 loss: 0.0098 lr: 0.005\n","iteration: 5040 loss: 0.0092 lr: 0.005\n","iteration: 5050 loss: 0.0108 lr: 0.005\n","iteration: 5060 loss: 0.0103 lr: 0.005\n","iteration: 5070 loss: 0.0099 lr: 0.005\n","iteration: 5080 loss: 0.0102 lr: 0.005\n","iteration: 5090 loss: 0.0108 lr: 0.005\n","iteration: 5100 loss: 0.0127 lr: 0.005\n","iteration: 5110 loss: 0.0098 lr: 0.005\n","iteration: 5120 loss: 0.0099 lr: 0.005\n","iteration: 5130 loss: 0.0113 lr: 0.005\n","iteration: 5140 loss: 0.0110 lr: 0.005\n","iteration: 5150 loss: 0.0105 lr: 0.005\n","iteration: 5160 loss: 0.0107 lr: 0.005\n","iteration: 5170 loss: 0.0109 lr: 0.005\n","iteration: 5180 loss: 0.0128 lr: 0.005\n","iteration: 5190 loss: 0.0113 lr: 0.005\n","iteration: 5200 loss: 0.0116 lr: 0.005\n","iteration: 5210 loss: 0.0109 lr: 0.005\n","iteration: 5220 loss: 0.0116 lr: 0.005\n","iteration: 5230 loss: 0.0106 lr: 0.005\n","iteration: 5240 loss: 0.0122 lr: 0.005\n","iteration: 5250 loss: 0.0109 lr: 0.005\n","iteration: 5260 loss: 0.0105 lr: 0.005\n","iteration: 5270 loss: 0.0101 lr: 0.005\n","iteration: 5280 loss: 0.0114 lr: 0.005\n","iteration: 5290 loss: 0.0106 lr: 0.005\n","iteration: 5300 loss: 0.0099 lr: 0.005\n","iteration: 5310 loss: 0.0110 lr: 0.005\n","iteration: 5320 loss: 0.0102 lr: 0.005\n","iteration: 5330 loss: 0.0103 lr: 0.005\n","iteration: 5340 loss: 0.0100 lr: 0.005\n","iteration: 5350 loss: 0.0107 lr: 0.005\n","iteration: 5360 loss: 0.0097 lr: 0.005\n","iteration: 5370 loss: 0.0102 lr: 0.005\n","iteration: 5380 loss: 0.0107 lr: 0.005\n","iteration: 5390 loss: 0.0100 lr: 0.005\n","iteration: 5400 loss: 0.0104 lr: 0.005\n","iteration: 5410 loss: 0.0100 lr: 0.005\n","iteration: 5420 loss: 0.0091 lr: 0.005\n","iteration: 5430 loss: 0.0107 lr: 0.005\n","iteration: 5440 loss: 0.0116 lr: 0.005\n","iteration: 5450 loss: 0.0105 lr: 0.005\n","iteration: 5460 loss: 0.0106 lr: 0.005\n","iteration: 5470 loss: 0.0104 lr: 0.005\n","iteration: 5480 loss: 0.0101 lr: 0.005\n","iteration: 5490 loss: 0.0091 lr: 0.005\n","iteration: 5500 loss: 0.0089 lr: 0.005\n","iteration: 5510 loss: 0.0096 lr: 0.005\n","iteration: 5520 loss: 0.0082 lr: 0.005\n","iteration: 5530 loss: 0.0101 lr: 0.005\n","iteration: 5540 loss: 0.0093 lr: 0.005\n","iteration: 5550 loss: 0.0096 lr: 0.005\n","iteration: 5560 loss: 0.0118 lr: 0.005\n","iteration: 5570 loss: 0.0104 lr: 0.005\n","iteration: 5580 loss: 0.0098 lr: 0.005\n","iteration: 5590 loss: 0.0093 lr: 0.005\n","iteration: 5600 loss: 0.0096 lr: 0.005\n","iteration: 5610 loss: 0.0104 lr: 0.005\n","iteration: 5620 loss: 0.0110 lr: 0.005\n","iteration: 5630 loss: 0.0097 lr: 0.005\n","iteration: 5640 loss: 0.0094 lr: 0.005\n","iteration: 5650 loss: 0.0104 lr: 0.005\n","iteration: 5660 loss: 0.0087 lr: 0.005\n","iteration: 5670 loss: 0.0122 lr: 0.005\n","iteration: 5680 loss: 0.0106 lr: 0.005\n","iteration: 5690 loss: 0.0097 lr: 0.005\n","iteration: 5700 loss: 0.0093 lr: 0.005\n","iteration: 5710 loss: 0.0113 lr: 0.005\n","iteration: 5720 loss: 0.0113 lr: 0.005\n","iteration: 5730 loss: 0.0100 lr: 0.005\n","iteration: 5740 loss: 0.0101 lr: 0.005\n","iteration: 5750 loss: 0.0096 lr: 0.005\n","iteration: 5760 loss: 0.0098 lr: 0.005\n","iteration: 5770 loss: 0.0094 lr: 0.005\n","iteration: 5780 loss: 0.0099 lr: 0.005\n","iteration: 5790 loss: 0.0091 lr: 0.005\n","iteration: 5800 loss: 0.0089 lr: 0.005\n","iteration: 5810 loss: 0.0103 lr: 0.005\n","iteration: 5820 loss: 0.0115 lr: 0.005\n","iteration: 5830 loss: 0.0095 lr: 0.005\n","iteration: 5840 loss: 0.0104 lr: 0.005\n","iteration: 5850 loss: 0.0107 lr: 0.005\n","iteration: 5860 loss: 0.0086 lr: 0.005\n","iteration: 5870 loss: 0.0094 lr: 0.005\n","iteration: 5880 loss: 0.0106 lr: 0.005\n","iteration: 5890 loss: 0.0097 lr: 0.005\n","iteration: 5900 loss: 0.0091 lr: 0.005\n","iteration: 5910 loss: 0.0105 lr: 0.005\n","iteration: 5920 loss: 0.0093 lr: 0.005\n","iteration: 5930 loss: 0.0107 lr: 0.005\n","iteration: 5940 loss: 0.0105 lr: 0.005\n","iteration: 5950 loss: 0.0107 lr: 0.005\n","iteration: 5960 loss: 0.0096 lr: 0.005\n","iteration: 5970 loss: 0.0094 lr: 0.005\n","iteration: 5980 loss: 0.0107 lr: 0.005\n","iteration: 5990 loss: 0.0091 lr: 0.005\n","iteration: 6000 loss: 0.0119 lr: 0.005\n","iteration: 6010 loss: 0.0092 lr: 0.005\n","iteration: 6020 loss: 0.0102 lr: 0.005\n","iteration: 6030 loss: 0.0093 lr: 0.005\n","iteration: 6040 loss: 0.0093 lr: 0.005\n","iteration: 6050 loss: 0.0102 lr: 0.005\n","iteration: 6060 loss: 0.0105 lr: 0.005\n","iteration: 6070 loss: 0.0093 lr: 0.005\n","iteration: 6080 loss: 0.0104 lr: 0.005\n","iteration: 6090 loss: 0.0105 lr: 0.005\n","iteration: 6100 loss: 0.0104 lr: 0.005\n","iteration: 6110 loss: 0.0098 lr: 0.005\n","iteration: 6120 loss: 0.0088 lr: 0.005\n","iteration: 6130 loss: 0.0100 lr: 0.005\n","iteration: 6140 loss: 0.0116 lr: 0.005\n","iteration: 6150 loss: 0.0093 lr: 0.005\n","iteration: 6160 loss: 0.0096 lr: 0.005\n","iteration: 6170 loss: 0.0094 lr: 0.005\n","iteration: 6180 loss: 0.0083 lr: 0.005\n","iteration: 6190 loss: 0.0117 lr: 0.005\n","iteration: 6200 loss: 0.0096 lr: 0.005\n","iteration: 6210 loss: 0.0114 lr: 0.005\n","iteration: 6220 loss: 0.0104 lr: 0.005\n","iteration: 6230 loss: 0.0099 lr: 0.005\n","iteration: 6240 loss: 0.0088 lr: 0.005\n","iteration: 6250 loss: 0.0093 lr: 0.005\n","iteration: 6260 loss: 0.0094 lr: 0.005\n","iteration: 6270 loss: 0.0090 lr: 0.005\n","iteration: 6280 loss: 0.0096 lr: 0.005\n","iteration: 6290 loss: 0.0091 lr: 0.005\n","iteration: 6300 loss: 0.0106 lr: 0.005\n","iteration: 6310 loss: 0.0100 lr: 0.005\n","iteration: 6320 loss: 0.0094 lr: 0.005\n","iteration: 6330 loss: 0.0086 lr: 0.005\n","iteration: 6340 loss: 0.0095 lr: 0.005\n","iteration: 6350 loss: 0.0080 lr: 0.005\n","iteration: 6360 loss: 0.0110 lr: 0.005\n","iteration: 6370 loss: 0.0097 lr: 0.005\n","iteration: 6380 loss: 0.0095 lr: 0.005\n","iteration: 6390 loss: 0.0093 lr: 0.005\n","iteration: 6400 loss: 0.0098 lr: 0.005\n","iteration: 6410 loss: 0.0094 lr: 0.005\n","iteration: 6420 loss: 0.0093 lr: 0.005\n","iteration: 6430 loss: 0.0110 lr: 0.005\n","iteration: 6440 loss: 0.0104 lr: 0.005\n","iteration: 6450 loss: 0.0105 lr: 0.005\n","iteration: 6460 loss: 0.0099 lr: 0.005\n","iteration: 6470 loss: 0.0097 lr: 0.005\n","iteration: 6480 loss: 0.0093 lr: 0.005\n","iteration: 6490 loss: 0.0093 lr: 0.005\n","iteration: 6500 loss: 0.0100 lr: 0.005\n","iteration: 6510 loss: 0.0100 lr: 0.005\n","iteration: 6520 loss: 0.0102 lr: 0.005\n","iteration: 6530 loss: 0.0102 lr: 0.005\n","iteration: 6540 loss: 0.0088 lr: 0.005\n","iteration: 6550 loss: 0.0098 lr: 0.005\n","iteration: 6560 loss: 0.0098 lr: 0.005\n","iteration: 6570 loss: 0.0102 lr: 0.005\n","iteration: 6580 loss: 0.0109 lr: 0.005\n","iteration: 6590 loss: 0.0103 lr: 0.005\n","iteration: 6600 loss: 0.0105 lr: 0.005\n","iteration: 6610 loss: 0.0107 lr: 0.005\n","iteration: 6620 loss: 0.0101 lr: 0.005\n","iteration: 6630 loss: 0.0089 lr: 0.005\n","iteration: 6640 loss: 0.0091 lr: 0.005\n","iteration: 6650 loss: 0.0099 lr: 0.005\n","iteration: 6660 loss: 0.0097 lr: 0.005\n","iteration: 6670 loss: 0.0086 lr: 0.005\n","iteration: 6680 loss: 0.0091 lr: 0.005\n","iteration: 6690 loss: 0.0091 lr: 0.005\n","iteration: 6700 loss: 0.0105 lr: 0.005\n","iteration: 6710 loss: 0.0092 lr: 0.005\n","iteration: 6720 loss: 0.0094 lr: 0.005\n","iteration: 6730 loss: 0.0105 lr: 0.005\n","iteration: 6740 loss: 0.0097 lr: 0.005\n","iteration: 6750 loss: 0.0092 lr: 0.005\n","iteration: 6760 loss: 0.0091 lr: 0.005\n","iteration: 6770 loss: 0.0087 lr: 0.005\n","iteration: 6780 loss: 0.0105 lr: 0.005\n","iteration: 6790 loss: 0.0090 lr: 0.005\n","iteration: 6800 loss: 0.0102 lr: 0.005\n","iteration: 6810 loss: 0.0094 lr: 0.005\n","iteration: 6820 loss: 0.0093 lr: 0.005\n","iteration: 6830 loss: 0.0102 lr: 0.005\n","iteration: 6840 loss: 0.0090 lr: 0.005\n","iteration: 6850 loss: 0.0098 lr: 0.005\n","iteration: 6860 loss: 0.0088 lr: 0.005\n","iteration: 6870 loss: 0.0100 lr: 0.005\n","iteration: 6880 loss: 0.0105 lr: 0.005\n","iteration: 6890 loss: 0.0104 lr: 0.005\n","iteration: 6900 loss: 0.0085 lr: 0.005\n","iteration: 6910 loss: 0.0083 lr: 0.005\n","iteration: 6920 loss: 0.0079 lr: 0.005\n","iteration: 6930 loss: 0.0101 lr: 0.005\n","iteration: 6940 loss: 0.0098 lr: 0.005\n","iteration: 6950 loss: 0.0106 lr: 0.005\n","iteration: 6960 loss: 0.0091 lr: 0.005\n","iteration: 6970 loss: 0.0087 lr: 0.005\n","iteration: 6980 loss: 0.0096 lr: 0.005\n","iteration: 6990 loss: 0.0096 lr: 0.005\n","iteration: 7000 loss: 0.0093 lr: 0.005\n","iteration: 7010 loss: 0.0087 lr: 0.005\n","iteration: 7020 loss: 0.0113 lr: 0.005\n","iteration: 7030 loss: 0.0115 lr: 0.005\n","iteration: 7040 loss: 0.0097 lr: 0.005\n","iteration: 7050 loss: 0.0099 lr: 0.005\n","iteration: 7060 loss: 0.0107 lr: 0.005\n","iteration: 7070 loss: 0.0088 lr: 0.005\n","iteration: 7080 loss: 0.0084 lr: 0.005\n","iteration: 7090 loss: 0.0086 lr: 0.005\n","iteration: 7100 loss: 0.0093 lr: 0.005\n","iteration: 7110 loss: 0.0101 lr: 0.005\n","iteration: 7120 loss: 0.0089 lr: 0.005\n","iteration: 7130 loss: 0.0105 lr: 0.005\n","iteration: 7140 loss: 0.0088 lr: 0.005\n","iteration: 7150 loss: 0.0099 lr: 0.005\n","iteration: 7160 loss: 0.0092 lr: 0.005\n","iteration: 7170 loss: 0.0092 lr: 0.005\n","iteration: 7180 loss: 0.0105 lr: 0.005\n","iteration: 7190 loss: 0.0094 lr: 0.005\n","iteration: 7200 loss: 0.0096 lr: 0.005\n","iteration: 7210 loss: 0.0099 lr: 0.005\n","iteration: 7220 loss: 0.0098 lr: 0.005\n","iteration: 7230 loss: 0.0100 lr: 0.005\n","iteration: 7240 loss: 0.0105 lr: 0.005\n","iteration: 7250 loss: 0.0093 lr: 0.005\n","iteration: 7260 loss: 0.0098 lr: 0.005\n","iteration: 7270 loss: 0.0082 lr: 0.005\n","iteration: 7280 loss: 0.0101 lr: 0.005\n","iteration: 7290 loss: 0.0094 lr: 0.005\n","iteration: 7300 loss: 0.0093 lr: 0.005\n","iteration: 7310 loss: 0.0094 lr: 0.005\n","iteration: 7320 loss: 0.0088 lr: 0.005\n","iteration: 7330 loss: 0.0086 lr: 0.005\n","iteration: 7340 loss: 0.0099 lr: 0.005\n","iteration: 7350 loss: 0.0082 lr: 0.005\n","iteration: 7360 loss: 0.0098 lr: 0.005\n","iteration: 7370 loss: 0.0095 lr: 0.005\n","iteration: 7380 loss: 0.0087 lr: 0.005\n","iteration: 7390 loss: 0.0079 lr: 0.005\n","iteration: 7400 loss: 0.0078 lr: 0.005\n","iteration: 7410 loss: 0.0100 lr: 0.005\n","iteration: 7420 loss: 0.0089 lr: 0.005\n","iteration: 7430 loss: 0.0106 lr: 0.005\n","iteration: 7440 loss: 0.0094 lr: 0.005\n","iteration: 7450 loss: 0.0087 lr: 0.005\n","iteration: 7460 loss: 0.0090 lr: 0.005\n","iteration: 7470 loss: 0.0089 lr: 0.005\n","iteration: 7480 loss: 0.0082 lr: 0.005\n","iteration: 7490 loss: 0.0087 lr: 0.005\n","iteration: 7500 loss: 0.0081 lr: 0.005\n","iteration: 7510 loss: 0.0090 lr: 0.005\n","iteration: 7520 loss: 0.0094 lr: 0.005\n","iteration: 7530 loss: 0.0101 lr: 0.005\n","iteration: 7540 loss: 0.0089 lr: 0.005\n","iteration: 7550 loss: 0.0094 lr: 0.005\n","iteration: 7560 loss: 0.0088 lr: 0.005\n","iteration: 7570 loss: 0.0086 lr: 0.005\n","iteration: 7580 loss: 0.0092 lr: 0.005\n","iteration: 7590 loss: 0.0097 lr: 0.005\n","iteration: 7600 loss: 0.0089 lr: 0.005\n","iteration: 7610 loss: 0.0105 lr: 0.005\n","iteration: 7620 loss: 0.0088 lr: 0.005\n","iteration: 7630 loss: 0.0082 lr: 0.005\n","iteration: 7640 loss: 0.0097 lr: 0.005\n","iteration: 7650 loss: 0.0081 lr: 0.005\n","iteration: 7660 loss: 0.0100 lr: 0.005\n","iteration: 7670 loss: 0.0084 lr: 0.005\n","iteration: 7680 loss: 0.0084 lr: 0.005\n","iteration: 7690 loss: 0.0093 lr: 0.005\n","iteration: 7700 loss: 0.0095 lr: 0.005\n","iteration: 7710 loss: 0.0102 lr: 0.005\n","iteration: 7720 loss: 0.0089 lr: 0.005\n","iteration: 7730 loss: 0.0093 lr: 0.005\n","iteration: 7740 loss: 0.0086 lr: 0.005\n","iteration: 7750 loss: 0.0087 lr: 0.005\n","iteration: 7760 loss: 0.0095 lr: 0.005\n","iteration: 7770 loss: 0.0089 lr: 0.005\n","iteration: 7780 loss: 0.0086 lr: 0.005\n","iteration: 7790 loss: 0.0082 lr: 0.005\n","iteration: 7800 loss: 0.0087 lr: 0.005\n","iteration: 7810 loss: 0.0091 lr: 0.005\n","iteration: 7820 loss: 0.0085 lr: 0.005\n","iteration: 7830 loss: 0.0088 lr: 0.005\n","iteration: 7840 loss: 0.0081 lr: 0.005\n","iteration: 7850 loss: 0.0080 lr: 0.005\n","iteration: 7860 loss: 0.0084 lr: 0.005\n","iteration: 7870 loss: 0.0098 lr: 0.005\n","iteration: 7880 loss: 0.0093 lr: 0.005\n","iteration: 7890 loss: 0.0086 lr: 0.005\n","iteration: 7900 loss: 0.0103 lr: 0.005\n","iteration: 7910 loss: 0.0107 lr: 0.005\n","iteration: 7920 loss: 0.0090 lr: 0.005\n","iteration: 7930 loss: 0.0089 lr: 0.005\n","iteration: 7940 loss: 0.0082 lr: 0.005\n","iteration: 7950 loss: 0.0084 lr: 0.005\n","iteration: 7960 loss: 0.0083 lr: 0.005\n","iteration: 7970 loss: 0.0090 lr: 0.005\n","iteration: 7980 loss: 0.0088 lr: 0.005\n","iteration: 7990 loss: 0.0099 lr: 0.005\n","iteration: 8000 loss: 0.0082 lr: 0.005\n","iteration: 8010 loss: 0.0079 lr: 0.005\n","iteration: 8020 loss: 0.0084 lr: 0.005\n","iteration: 8030 loss: 0.0094 lr: 0.005\n","iteration: 8040 loss: 0.0085 lr: 0.005\n","iteration: 8050 loss: 0.0095 lr: 0.005\n","iteration: 8060 loss: 0.0092 lr: 0.005\n","iteration: 8070 loss: 0.0096 lr: 0.005\n","iteration: 8080 loss: 0.0088 lr: 0.005\n","iteration: 8090 loss: 0.0088 lr: 0.005\n","iteration: 8100 loss: 0.0087 lr: 0.005\n","iteration: 8110 loss: 0.0088 lr: 0.005\n","iteration: 8120 loss: 0.0096 lr: 0.005\n","iteration: 8130 loss: 0.0090 lr: 0.005\n","iteration: 8140 loss: 0.0078 lr: 0.005\n","iteration: 8150 loss: 0.0085 lr: 0.005\n","iteration: 8160 loss: 0.0078 lr: 0.005\n","iteration: 8170 loss: 0.0084 lr: 0.005\n","iteration: 8180 loss: 0.0094 lr: 0.005\n","iteration: 8190 loss: 0.0086 lr: 0.005\n","iteration: 8200 loss: 0.0091 lr: 0.005\n","iteration: 8210 loss: 0.0079 lr: 0.005\n","iteration: 8220 loss: 0.0085 lr: 0.005\n","iteration: 8230 loss: 0.0086 lr: 0.005\n","iteration: 8240 loss: 0.0079 lr: 0.005\n","iteration: 8250 loss: 0.0088 lr: 0.005\n","iteration: 8260 loss: 0.0076 lr: 0.005\n","iteration: 8270 loss: 0.0088 lr: 0.005\n","iteration: 8280 loss: 0.0087 lr: 0.005\n","iteration: 8290 loss: 0.0079 lr: 0.005\n","iteration: 8300 loss: 0.0072 lr: 0.005\n","iteration: 8310 loss: 0.0098 lr: 0.005\n","iteration: 8320 loss: 0.0088 lr: 0.005\n","iteration: 8330 loss: 0.0072 lr: 0.005\n","iteration: 8340 loss: 0.0077 lr: 0.005\n","iteration: 8350 loss: 0.0076 lr: 0.005\n","iteration: 8360 loss: 0.0085 lr: 0.005\n","iteration: 8370 loss: 0.0114 lr: 0.005\n","iteration: 8380 loss: 0.0081 lr: 0.005\n","iteration: 8390 loss: 0.0095 lr: 0.005\n","iteration: 8400 loss: 0.0087 lr: 0.005\n","iteration: 8410 loss: 0.0090 lr: 0.005\n","iteration: 8420 loss: 0.0086 lr: 0.005\n","iteration: 8430 loss: 0.0090 lr: 0.005\n","iteration: 8440 loss: 0.0085 lr: 0.005\n","iteration: 8450 loss: 0.0090 lr: 0.005\n","iteration: 8460 loss: 0.0091 lr: 0.005\n","iteration: 8470 loss: 0.0080 lr: 0.005\n","iteration: 8480 loss: 0.0093 lr: 0.005\n","iteration: 8490 loss: 0.0084 lr: 0.005\n","iteration: 8500 loss: 0.0080 lr: 0.005\n","iteration: 8510 loss: 0.0081 lr: 0.005\n","iteration: 8520 loss: 0.0083 lr: 0.005\n","iteration: 8530 loss: 0.0081 lr: 0.005\n","iteration: 8540 loss: 0.0076 lr: 0.005\n","iteration: 8550 loss: 0.0106 lr: 0.005\n","iteration: 8560 loss: 0.0084 lr: 0.005\n","iteration: 8570 loss: 0.0079 lr: 0.005\n","iteration: 8580 loss: 0.0079 lr: 0.005\n","iteration: 8590 loss: 0.0089 lr: 0.005\n","iteration: 8600 loss: 0.0087 lr: 0.005\n","iteration: 8610 loss: 0.0085 lr: 0.005\n","iteration: 8620 loss: 0.0072 lr: 0.005\n","iteration: 8630 loss: 0.0071 lr: 0.005\n","iteration: 8640 loss: 0.0089 lr: 0.005\n","iteration: 8650 loss: 0.0084 lr: 0.005\n","iteration: 8660 loss: 0.0089 lr: 0.005\n","iteration: 8670 loss: 0.0088 lr: 0.005\n","iteration: 8680 loss: 0.0077 lr: 0.005\n","iteration: 8690 loss: 0.0091 lr: 0.005\n","iteration: 8700 loss: 0.0091 lr: 0.005\n","iteration: 8710 loss: 0.0082 lr: 0.005\n","iteration: 8720 loss: 0.0095 lr: 0.005\n","iteration: 8730 loss: 0.0076 lr: 0.005\n","iteration: 8740 loss: 0.0082 lr: 0.005\n","iteration: 8750 loss: 0.0091 lr: 0.005\n","iteration: 8760 loss: 0.0085 lr: 0.005\n","iteration: 8770 loss: 0.0082 lr: 0.005\n","iteration: 8780 loss: 0.0085 lr: 0.005\n","iteration: 8790 loss: 0.0092 lr: 0.005\n","iteration: 8800 loss: 0.0087 lr: 0.005\n","iteration: 8810 loss: 0.0076 lr: 0.005\n","iteration: 8820 loss: 0.0082 lr: 0.005\n","iteration: 8830 loss: 0.0079 lr: 0.005\n","iteration: 8840 loss: 0.0093 lr: 0.005\n","iteration: 8850 loss: 0.0080 lr: 0.005\n","iteration: 8860 loss: 0.0082 lr: 0.005\n","iteration: 8870 loss: 0.0081 lr: 0.005\n","iteration: 8880 loss: 0.0074 lr: 0.005\n","iteration: 8890 loss: 0.0094 lr: 0.005\n","iteration: 8900 loss: 0.0082 lr: 0.005\n","iteration: 8910 loss: 0.0083 lr: 0.005\n","iteration: 8920 loss: 0.0080 lr: 0.005\n","iteration: 8930 loss: 0.0093 lr: 0.005\n","iteration: 8940 loss: 0.0079 lr: 0.005\n","iteration: 8950 loss: 0.0078 lr: 0.005\n","iteration: 8960 loss: 0.0071 lr: 0.005\n","iteration: 8970 loss: 0.0077 lr: 0.005\n","iteration: 8980 loss: 0.0075 lr: 0.005\n","iteration: 8990 loss: 0.0078 lr: 0.005\n","iteration: 9000 loss: 0.0086 lr: 0.005\n","iteration: 9010 loss: 0.0097 lr: 0.005\n","iteration: 9020 loss: 0.0087 lr: 0.005\n","iteration: 9030 loss: 0.0093 lr: 0.005\n","iteration: 9040 loss: 0.0087 lr: 0.005\n","iteration: 9050 loss: 0.0090 lr: 0.005\n","iteration: 9060 loss: 0.0079 lr: 0.005\n","iteration: 9070 loss: 0.0078 lr: 0.005\n","iteration: 9080 loss: 0.0095 lr: 0.005\n","iteration: 9090 loss: 0.0087 lr: 0.005\n","iteration: 9100 loss: 0.0075 lr: 0.005\n","iteration: 9110 loss: 0.0075 lr: 0.005\n","iteration: 9120 loss: 0.0082 lr: 0.005\n","iteration: 9130 loss: 0.0086 lr: 0.005\n","iteration: 9140 loss: 0.0088 lr: 0.005\n","iteration: 9150 loss: 0.0083 lr: 0.005\n","iteration: 9160 loss: 0.0072 lr: 0.005\n","iteration: 9170 loss: 0.0087 lr: 0.005\n","iteration: 9180 loss: 0.0078 lr: 0.005\n","iteration: 9190 loss: 0.0084 lr: 0.005\n","iteration: 9200 loss: 0.0081 lr: 0.005\n","iteration: 9210 loss: 0.0085 lr: 0.005\n","iteration: 9220 loss: 0.0081 lr: 0.005\n","iteration: 9230 loss: 0.0087 lr: 0.005\n","iteration: 9240 loss: 0.0080 lr: 0.005\n","iteration: 9250 loss: 0.0075 lr: 0.005\n","iteration: 9260 loss: 0.0078 lr: 0.005\n","iteration: 9270 loss: 0.0085 lr: 0.005\n","iteration: 9280 loss: 0.0078 lr: 0.005\n","iteration: 9290 loss: 0.0087 lr: 0.005\n","iteration: 9300 loss: 0.0084 lr: 0.005\n","iteration: 9310 loss: 0.0068 lr: 0.005\n","iteration: 9320 loss: 0.0071 lr: 0.005\n","iteration: 9330 loss: 0.0094 lr: 0.005\n","iteration: 9340 loss: 0.0087 lr: 0.005\n","iteration: 9350 loss: 0.0091 lr: 0.005\n","iteration: 9360 loss: 0.0100 lr: 0.005\n","iteration: 9370 loss: 0.0089 lr: 0.005\n","iteration: 9380 loss: 0.0086 lr: 0.005\n","iteration: 9390 loss: 0.0079 lr: 0.005\n","iteration: 9400 loss: 0.0074 lr: 0.005\n","iteration: 9410 loss: 0.0080 lr: 0.005\n","iteration: 9420 loss: 0.0089 lr: 0.005\n","iteration: 9430 loss: 0.0093 lr: 0.005\n","iteration: 9440 loss: 0.0091 lr: 0.005\n","iteration: 9450 loss: 0.0083 lr: 0.005\n","iteration: 9460 loss: 0.0068 lr: 0.005\n","iteration: 9470 loss: 0.0063 lr: 0.005\n","iteration: 9480 loss: 0.0075 lr: 0.005\n","iteration: 9490 loss: 0.0083 lr: 0.005\n","iteration: 9500 loss: 0.0074 lr: 0.005\n","iteration: 9510 loss: 0.0088 lr: 0.005\n","iteration: 9520 loss: 0.0087 lr: 0.005\n","iteration: 9530 loss: 0.0073 lr: 0.005\n","iteration: 9540 loss: 0.0073 lr: 0.005\n","iteration: 9550 loss: 0.0076 lr: 0.005\n","iteration: 9560 loss: 0.0086 lr: 0.005\n","iteration: 9570 loss: 0.0076 lr: 0.005\n","iteration: 9580 loss: 0.0076 lr: 0.005\n","iteration: 9590 loss: 0.0080 lr: 0.005\n","iteration: 9600 loss: 0.0094 lr: 0.005\n","iteration: 9610 loss: 0.0082 lr: 0.005\n","iteration: 9620 loss: 0.0083 lr: 0.005\n","iteration: 9630 loss: 0.0081 lr: 0.005\n","iteration: 9640 loss: 0.0079 lr: 0.005\n","iteration: 9650 loss: 0.0074 lr: 0.005\n","iteration: 9660 loss: 0.0082 lr: 0.005\n","iteration: 9670 loss: 0.0086 lr: 0.005\n","iteration: 9680 loss: 0.0078 lr: 0.005\n","iteration: 9690 loss: 0.0080 lr: 0.005\n","iteration: 9700 loss: 0.0080 lr: 0.005\n","iteration: 9710 loss: 0.0077 lr: 0.005\n","iteration: 9720 loss: 0.0080 lr: 0.005\n","iteration: 9730 loss: 0.0077 lr: 0.005\n","iteration: 9740 loss: 0.0073 lr: 0.005\n","iteration: 9750 loss: 0.0066 lr: 0.005\n","iteration: 9760 loss: 0.0077 lr: 0.005\n","iteration: 9770 loss: 0.0072 lr: 0.005\n","iteration: 9780 loss: 0.0081 lr: 0.005\n","iteration: 9790 loss: 0.0085 lr: 0.005\n","iteration: 9800 loss: 0.0071 lr: 0.005\n","iteration: 9810 loss: 0.0065 lr: 0.005\n","iteration: 9820 loss: 0.0092 lr: 0.005\n","iteration: 9830 loss: 0.0087 lr: 0.005\n","iteration: 9840 loss: 0.0077 lr: 0.005\n","iteration: 9850 loss: 0.0081 lr: 0.005\n","iteration: 9860 loss: 0.0077 lr: 0.005\n","iteration: 9870 loss: 0.0072 lr: 0.005\n","iteration: 9880 loss: 0.0085 lr: 0.005\n","iteration: 9890 loss: 0.0073 lr: 0.005\n","iteration: 9900 loss: 0.0081 lr: 0.005\n","iteration: 9910 loss: 0.0086 lr: 0.005\n","iteration: 9920 loss: 0.0082 lr: 0.005\n","iteration: 9930 loss: 0.0075 lr: 0.005\n","iteration: 9940 loss: 0.0065 lr: 0.005\n","iteration: 9950 loss: 0.0067 lr: 0.005\n","iteration: 9960 loss: 0.0069 lr: 0.005\n","iteration: 9970 loss: 0.0086 lr: 0.005\n","iteration: 9980 loss: 0.0078 lr: 0.005\n","iteration: 9990 loss: 0.0069 lr: 0.005\n","iteration: 10000 loss: 0.0072 lr: 0.005\n","iteration: 10010 loss: 0.0092 lr: 0.02\n","iteration: 10020 loss: 0.0110 lr: 0.02\n","iteration: 10030 loss: 0.0108 lr: 0.02\n","iteration: 10040 loss: 0.0105 lr: 0.02\n","iteration: 10050 loss: 0.0107 lr: 0.02\n","iteration: 10060 loss: 0.0122 lr: 0.02\n","iteration: 10070 loss: 0.0124 lr: 0.02\n","iteration: 10080 loss: 0.0118 lr: 0.02\n","iteration: 10090 loss: 0.0114 lr: 0.02\n","iteration: 10100 loss: 0.0116 lr: 0.02\n","iteration: 10110 loss: 0.0118 lr: 0.02\n","iteration: 10120 loss: 0.0144 lr: 0.02\n","iteration: 10130 loss: 0.0131 lr: 0.02\n","iteration: 10140 loss: 0.0114 lr: 0.02\n","iteration: 10150 loss: 0.0100 lr: 0.02\n","iteration: 10160 loss: 0.0109 lr: 0.02\n","iteration: 10170 loss: 0.0109 lr: 0.02\n","iteration: 10180 loss: 0.0126 lr: 0.02\n","iteration: 10190 loss: 0.0118 lr: 0.02\n","iteration: 10200 loss: 0.0113 lr: 0.02\n","iteration: 10210 loss: 0.0095 lr: 0.02\n","iteration: 10220 loss: 0.0111 lr: 0.02\n","iteration: 10230 loss: 0.0108 lr: 0.02\n","iteration: 10240 loss: 0.0105 lr: 0.02\n","iteration: 10250 loss: 0.0098 lr: 0.02\n","iteration: 10260 loss: 0.0092 lr: 0.02\n","iteration: 10270 loss: 0.0112 lr: 0.02\n","iteration: 10280 loss: 0.0099 lr: 0.02\n","iteration: 10290 loss: 0.0118 lr: 0.02\n","iteration: 10300 loss: 0.0112 lr: 0.02\n","iteration: 10310 loss: 0.0105 lr: 0.02\n","iteration: 10320 loss: 0.0115 lr: 0.02\n","iteration: 10330 loss: 0.0110 lr: 0.02\n","iteration: 10340 loss: 0.0096 lr: 0.02\n","iteration: 10350 loss: 0.0097 lr: 0.02\n","iteration: 10360 loss: 0.0101 lr: 0.02\n","iteration: 10370 loss: 0.0108 lr: 0.02\n","iteration: 10380 loss: 0.0099 lr: 0.02\n","iteration: 10390 loss: 0.0107 lr: 0.02\n","iteration: 10400 loss: 0.0107 lr: 0.02\n","iteration: 10410 loss: 0.0091 lr: 0.02\n","iteration: 10420 loss: 0.0095 lr: 0.02\n","iteration: 10430 loss: 0.0097 lr: 0.02\n","iteration: 10440 loss: 0.0104 lr: 0.02\n","iteration: 10450 loss: 0.0093 lr: 0.02\n","iteration: 10460 loss: 0.0098 lr: 0.02\n","iteration: 10470 loss: 0.0093 lr: 0.02\n","iteration: 10480 loss: 0.0100 lr: 0.02\n","iteration: 10490 loss: 0.0089 lr: 0.02\n","iteration: 10500 loss: 0.0114 lr: 0.02\n","iteration: 10510 loss: 0.0104 lr: 0.02\n","iteration: 10520 loss: 0.0102 lr: 0.02\n","iteration: 10530 loss: 0.0109 lr: 0.02\n","iteration: 10540 loss: 0.0104 lr: 0.02\n","iteration: 10550 loss: 0.0102 lr: 0.02\n","iteration: 10560 loss: 0.0091 lr: 0.02\n","iteration: 10570 loss: 0.0096 lr: 0.02\n","iteration: 10580 loss: 0.0107 lr: 0.02\n","iteration: 10590 loss: 0.0089 lr: 0.02\n","iteration: 10600 loss: 0.0095 lr: 0.02\n","iteration: 10610 loss: 0.0089 lr: 0.02\n","iteration: 10620 loss: 0.0099 lr: 0.02\n","iteration: 10630 loss: 0.0085 lr: 0.02\n","iteration: 10640 loss: 0.0089 lr: 0.02\n","iteration: 10650 loss: 0.0090 lr: 0.02\n","iteration: 10660 loss: 0.0100 lr: 0.02\n","iteration: 10670 loss: 0.0101 lr: 0.02\n","iteration: 10680 loss: 0.0095 lr: 0.02\n","iteration: 10690 loss: 0.0095 lr: 0.02\n","iteration: 10700 loss: 0.0112 lr: 0.02\n","iteration: 10710 loss: 0.0098 lr: 0.02\n","iteration: 10720 loss: 0.0102 lr: 0.02\n","iteration: 10730 loss: 0.0090 lr: 0.02\n","iteration: 10740 loss: 0.0100 lr: 0.02\n","iteration: 10750 loss: 0.0097 lr: 0.02\n","iteration: 10760 loss: 0.0100 lr: 0.02\n","iteration: 10770 loss: 0.0097 lr: 0.02\n","iteration: 10780 loss: 0.0084 lr: 0.02\n","iteration: 10790 loss: 0.0086 lr: 0.02\n","iteration: 10800 loss: 0.0085 lr: 0.02\n","iteration: 10810 loss: 0.0095 lr: 0.02\n","iteration: 10820 loss: 0.0099 lr: 0.02\n","iteration: 10830 loss: 0.0093 lr: 0.02\n","iteration: 10840 loss: 0.0076 lr: 0.02\n","iteration: 10850 loss: 0.0079 lr: 0.02\n","iteration: 10860 loss: 0.0104 lr: 0.02\n","iteration: 10870 loss: 0.0087 lr: 0.02\n","iteration: 10880 loss: 0.0106 lr: 0.02\n","iteration: 10890 loss: 0.0092 lr: 0.02\n","iteration: 10900 loss: 0.0092 lr: 0.02\n","iteration: 10910 loss: 0.0078 lr: 0.02\n","iteration: 10920 loss: 0.0092 lr: 0.02\n","iteration: 10930 loss: 0.0092 lr: 0.02\n","iteration: 10940 loss: 0.0097 lr: 0.02\n","iteration: 10950 loss: 0.0103 lr: 0.02\n","iteration: 10960 loss: 0.0097 lr: 0.02\n","iteration: 10970 loss: 0.0096 lr: 0.02\n","iteration: 10980 loss: 0.0100 lr: 0.02\n","iteration: 10990 loss: 0.0093 lr: 0.02\n","iteration: 11000 loss: 0.0090 lr: 0.02\n","iteration: 11010 loss: 0.0085 lr: 0.02\n","iteration: 11020 loss: 0.0109 lr: 0.02\n","iteration: 11030 loss: 0.0090 lr: 0.02\n","iteration: 11040 loss: 0.0083 lr: 0.02\n","iteration: 11050 loss: 0.0085 lr: 0.02\n","iteration: 11060 loss: 0.0099 lr: 0.02\n","iteration: 11070 loss: 0.0091 lr: 0.02\n","iteration: 11080 loss: 0.0081 lr: 0.02\n","iteration: 11090 loss: 0.0084 lr: 0.02\n","iteration: 11100 loss: 0.0092 lr: 0.02\n","iteration: 11110 loss: 0.0082 lr: 0.02\n","iteration: 11120 loss: 0.0085 lr: 0.02\n","iteration: 11130 loss: 0.0094 lr: 0.02\n","iteration: 11140 loss: 0.0086 lr: 0.02\n","iteration: 11150 loss: 0.0088 lr: 0.02\n","iteration: 11160 loss: 0.0080 lr: 0.02\n","iteration: 11170 loss: 0.0092 lr: 0.02\n","iteration: 11180 loss: 0.0087 lr: 0.02\n","iteration: 11190 loss: 0.0089 lr: 0.02\n","iteration: 11200 loss: 0.0079 lr: 0.02\n","iteration: 11210 loss: 0.0089 lr: 0.02\n","iteration: 11220 loss: 0.0080 lr: 0.02\n","iteration: 11230 loss: 0.0089 lr: 0.02\n","iteration: 11240 loss: 0.0083 lr: 0.02\n","iteration: 11250 loss: 0.0098 lr: 0.02\n","iteration: 11260 loss: 0.0100 lr: 0.02\n","iteration: 11270 loss: 0.0092 lr: 0.02\n","iteration: 11280 loss: 0.0090 lr: 0.02\n","iteration: 11290 loss: 0.0091 lr: 0.02\n","iteration: 11300 loss: 0.0081 lr: 0.02\n","iteration: 11310 loss: 0.0083 lr: 0.02\n","iteration: 11320 loss: 0.0080 lr: 0.02\n","iteration: 11330 loss: 0.0090 lr: 0.02\n","iteration: 11340 loss: 0.0086 lr: 0.02\n","iteration: 11350 loss: 0.0092 lr: 0.02\n","iteration: 11360 loss: 0.0093 lr: 0.02\n","iteration: 11370 loss: 0.0093 lr: 0.02\n","iteration: 11380 loss: 0.0083 lr: 0.02\n","iteration: 11390 loss: 0.0076 lr: 0.02\n","iteration: 11400 loss: 0.0076 lr: 0.02\n","iteration: 11410 loss: 0.0070 lr: 0.02\n","iteration: 11420 loss: 0.0080 lr: 0.02\n","iteration: 11430 loss: 0.0082 lr: 0.02\n","iteration: 11440 loss: 0.0082 lr: 0.02\n","iteration: 11450 loss: 0.0074 lr: 0.02\n","iteration: 11460 loss: 0.0077 lr: 0.02\n","iteration: 11470 loss: 0.0095 lr: 0.02\n","iteration: 11480 loss: 0.0078 lr: 0.02\n","iteration: 11490 loss: 0.0075 lr: 0.02\n","iteration: 11500 loss: 0.0080 lr: 0.02\n","iteration: 11510 loss: 0.0079 lr: 0.02\n","iteration: 11520 loss: 0.0082 lr: 0.02\n","iteration: 11530 loss: 0.0086 lr: 0.02\n","iteration: 11540 loss: 0.0078 lr: 0.02\n","iteration: 11550 loss: 0.0074 lr: 0.02\n","iteration: 11560 loss: 0.0078 lr: 0.02\n","iteration: 11570 loss: 0.0078 lr: 0.02\n","iteration: 11580 loss: 0.0079 lr: 0.02\n","iteration: 11590 loss: 0.0088 lr: 0.02\n","iteration: 11600 loss: 0.0075 lr: 0.02\n","iteration: 11610 loss: 0.0076 lr: 0.02\n","iteration: 11620 loss: 0.0081 lr: 0.02\n","iteration: 11630 loss: 0.0075 lr: 0.02\n","iteration: 11640 loss: 0.0076 lr: 0.02\n","iteration: 11650 loss: 0.0075 lr: 0.02\n","iteration: 11660 loss: 0.0092 lr: 0.02\n","iteration: 11670 loss: 0.0080 lr: 0.02\n","iteration: 11680 loss: 0.0079 lr: 0.02\n","iteration: 11690 loss: 0.0079 lr: 0.02\n","iteration: 11700 loss: 0.0076 lr: 0.02\n","iteration: 11710 loss: 0.0080 lr: 0.02\n","iteration: 11720 loss: 0.0085 lr: 0.02\n","iteration: 11730 loss: 0.0094 lr: 0.02\n","iteration: 11740 loss: 0.0076 lr: 0.02\n","iteration: 11750 loss: 0.0084 lr: 0.02\n","iteration: 11760 loss: 0.0080 lr: 0.02\n","iteration: 11770 loss: 0.0077 lr: 0.02\n","iteration: 11780 loss: 0.0080 lr: 0.02\n","iteration: 11790 loss: 0.0073 lr: 0.02\n","iteration: 11800 loss: 0.0074 lr: 0.02\n","iteration: 11810 loss: 0.0072 lr: 0.02\n","iteration: 11820 loss: 0.0089 lr: 0.02\n","iteration: 11830 loss: 0.0085 lr: 0.02\n","iteration: 11840 loss: 0.0079 lr: 0.02\n","iteration: 11850 loss: 0.0078 lr: 0.02\n","iteration: 11860 loss: 0.0085 lr: 0.02\n","iteration: 11870 loss: 0.0081 lr: 0.02\n","iteration: 11880 loss: 0.0071 lr: 0.02\n","iteration: 11890 loss: 0.0080 lr: 0.02\n","iteration: 11900 loss: 0.0073 lr: 0.02\n","iteration: 11910 loss: 0.0069 lr: 0.02\n","iteration: 11920 loss: 0.0069 lr: 0.02\n","iteration: 11930 loss: 0.0070 lr: 0.02\n","iteration: 11940 loss: 0.0080 lr: 0.02\n","iteration: 11950 loss: 0.0076 lr: 0.02\n","iteration: 11960 loss: 0.0077 lr: 0.02\n","iteration: 11970 loss: 0.0076 lr: 0.02\n","iteration: 11980 loss: 0.0078 lr: 0.02\n","iteration: 11990 loss: 0.0074 lr: 0.02\n","iteration: 12000 loss: 0.0073 lr: 0.02\n","iteration: 12010 loss: 0.0080 lr: 0.02\n","iteration: 12020 loss: 0.0084 lr: 0.02\n","iteration: 12030 loss: 0.0077 lr: 0.02\n","iteration: 12040 loss: 0.0069 lr: 0.02\n","iteration: 12050 loss: 0.0077 lr: 0.02\n","iteration: 12060 loss: 0.0080 lr: 0.02\n","iteration: 12070 loss: 0.0075 lr: 0.02\n","iteration: 12080 loss: 0.0075 lr: 0.02\n","iteration: 12090 loss: 0.0078 lr: 0.02\n","iteration: 12100 loss: 0.0078 lr: 0.02\n","iteration: 12110 loss: 0.0077 lr: 0.02\n","iteration: 12120 loss: 0.0079 lr: 0.02\n","iteration: 12130 loss: 0.0071 lr: 0.02\n","iteration: 12140 loss: 0.0088 lr: 0.02\n","iteration: 12150 loss: 0.0076 lr: 0.02\n","iteration: 12160 loss: 0.0079 lr: 0.02\n","iteration: 12170 loss: 0.0077 lr: 0.02\n","iteration: 12180 loss: 0.0081 lr: 0.02\n","iteration: 12190 loss: 0.0066 lr: 0.02\n","iteration: 12200 loss: 0.0079 lr: 0.02\n","iteration: 12210 loss: 0.0076 lr: 0.02\n","iteration: 12220 loss: 0.0076 lr: 0.02\n","iteration: 12230 loss: 0.0071 lr: 0.02\n","iteration: 12240 loss: 0.0069 lr: 0.02\n","iteration: 12250 loss: 0.0076 lr: 0.02\n","iteration: 12260 loss: 0.0072 lr: 0.02\n","iteration: 12270 loss: 0.0063 lr: 0.02\n","iteration: 12280 loss: 0.0076 lr: 0.02\n","iteration: 12290 loss: 0.0068 lr: 0.02\n","iteration: 12300 loss: 0.0064 lr: 0.02\n","iteration: 12310 loss: 0.0082 lr: 0.02\n","iteration: 12320 loss: 0.0079 lr: 0.02\n","iteration: 12330 loss: 0.0075 lr: 0.02\n","iteration: 12340 loss: 0.0080 lr: 0.02\n","iteration: 12350 loss: 0.0078 lr: 0.02\n","iteration: 12360 loss: 0.0086 lr: 0.02\n","iteration: 12370 loss: 0.0081 lr: 0.02\n","iteration: 12380 loss: 0.0071 lr: 0.02\n","iteration: 12390 loss: 0.0080 lr: 0.02\n","iteration: 12400 loss: 0.0081 lr: 0.02\n","iteration: 12410 loss: 0.0071 lr: 0.02\n","iteration: 12420 loss: 0.0068 lr: 0.02\n","iteration: 12430 loss: 0.0074 lr: 0.02\n","iteration: 12440 loss: 0.0074 lr: 0.02\n","iteration: 12450 loss: 0.0069 lr: 0.02\n","iteration: 12460 loss: 0.0070 lr: 0.02\n","iteration: 12470 loss: 0.0086 lr: 0.02\n","iteration: 12480 loss: 0.0070 lr: 0.02\n","iteration: 12490 loss: 0.0077 lr: 0.02\n","iteration: 12500 loss: 0.0086 lr: 0.02\n","iteration: 12510 loss: 0.0078 lr: 0.02\n","iteration: 12520 loss: 0.0071 lr: 0.02\n","iteration: 12530 loss: 0.0081 lr: 0.02\n","iteration: 12540 loss: 0.0079 lr: 0.02\n","iteration: 12550 loss: 0.0080 lr: 0.02\n","iteration: 12560 loss: 0.0073 lr: 0.02\n","iteration: 12570 loss: 0.0070 lr: 0.02\n","iteration: 12580 loss: 0.0068 lr: 0.02\n","iteration: 12590 loss: 0.0068 lr: 0.02\n","iteration: 12600 loss: 0.0069 lr: 0.02\n","iteration: 12610 loss: 0.0067 lr: 0.02\n","iteration: 12620 loss: 0.0064 lr: 0.02\n","iteration: 12630 loss: 0.0079 lr: 0.02\n","iteration: 12640 loss: 0.0081 lr: 0.02\n","iteration: 12650 loss: 0.0065 lr: 0.02\n","iteration: 12660 loss: 0.0069 lr: 0.02\n","iteration: 12670 loss: 0.0069 lr: 0.02\n","iteration: 12680 loss: 0.0072 lr: 0.02\n","iteration: 12690 loss: 0.0068 lr: 0.02\n","iteration: 12700 loss: 0.0075 lr: 0.02\n","iteration: 12710 loss: 0.0071 lr: 0.02\n","iteration: 12720 loss: 0.0075 lr: 0.02\n","iteration: 12730 loss: 0.0082 lr: 0.02\n","iteration: 12740 loss: 0.0075 lr: 0.02\n","iteration: 12750 loss: 0.0078 lr: 0.02\n","iteration: 12760 loss: 0.0084 lr: 0.02\n","iteration: 12770 loss: 0.0080 lr: 0.02\n","iteration: 12780 loss: 0.0078 lr: 0.02\n","iteration: 12790 loss: 0.0087 lr: 0.02\n","iteration: 12800 loss: 0.0073 lr: 0.02\n","iteration: 12810 loss: 0.0074 lr: 0.02\n","iteration: 12820 loss: 0.0089 lr: 0.02\n","iteration: 12830 loss: 0.0073 lr: 0.02\n","iteration: 12840 loss: 0.0076 lr: 0.02\n","iteration: 12850 loss: 0.0072 lr: 0.02\n","iteration: 12860 loss: 0.0066 lr: 0.02\n","iteration: 12870 loss: 0.0069 lr: 0.02\n","iteration: 12880 loss: 0.0073 lr: 0.02\n","iteration: 12890 loss: 0.0072 lr: 0.02\n","iteration: 12900 loss: 0.0068 lr: 0.02\n","iteration: 12910 loss: 0.0067 lr: 0.02\n","iteration: 12920 loss: 0.0072 lr: 0.02\n","iteration: 12930 loss: 0.0072 lr: 0.02\n","iteration: 12940 loss: 0.0068 lr: 0.02\n","iteration: 12950 loss: 0.0071 lr: 0.02\n","iteration: 12960 loss: 0.0084 lr: 0.02\n","iteration: 12970 loss: 0.0066 lr: 0.02\n","iteration: 12980 loss: 0.0073 lr: 0.02\n","iteration: 12990 loss: 0.0066 lr: 0.02\n","iteration: 13000 loss: 0.0073 lr: 0.02\n","iteration: 13010 loss: 0.0067 lr: 0.02\n","iteration: 13020 loss: 0.0068 lr: 0.02\n","iteration: 13030 loss: 0.0074 lr: 0.02\n","iteration: 13040 loss: 0.0065 lr: 0.02\n","iteration: 13050 loss: 0.0077 lr: 0.02\n","iteration: 13060 loss: 0.0064 lr: 0.02\n","iteration: 13070 loss: 0.0071 lr: 0.02\n"]}],"source":["#let's also change the display and save_iters just in case Colab takes away the GPU... \n","#if that happens, you can reload from a saved point. Typically, you want to train to 200,000 + iterations.\n","#more info and there are more things you can set: https://github.com/AlexEMG/DeepLabCut/blob/master/docs/functionDetails.md#g-train-the-network\n","\n","deeplabcut.train_network(path_config_file, shuffle=1, displayiters=10,saveiters=500)\n","\n","#this will run until you stop it (CTRL+C), or hit \"STOP\" icon, or when it hits the end (default, 1.03M iterations). \n","#Whichever you chose, you will see what looks like an error message, but it's not an error - don't worry...."]},{"cell_type":"markdown","metadata":{"id":"RiDwIVf5-3H_"},"source":["**When you hit \"STOP\" you will get a KeyInterrupt \"error\"! No worries! :)**"]},{"cell_type":"markdown","metadata":{"id":"xZygsb2DoEJc"},"source":["## Start evaluating:\n","This funtion evaluates a trained model for a specific shuffle/shuffles at a particular state or all the states on the data set (images)\n","and stores the results as .csv file in a subdirectory under **evaluation-results**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":236},"executionInfo":{"elapsed":388,"status":"error","timestamp":1646689877300,"user":{"displayName":"Raima Sen","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjPtYpIHHudDQRAtyYm8EgdjpP9gnylWwSSjx-q=s64","userId":"00263744560502469226"},"user_tz":300},"id":"nv4zlbrnoEJg","outputId":"9b70bc57-9033-4e0d-ff01-fc27cdea12ca"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-d4de8d002536>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib notebook'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdeeplabcut\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_config_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mplotting\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Here you want to see a low pixel error! Of course, it can only be as good as the labeler,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#so be sure your labels are good! (And you have trained enough ;)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'deeplabcut' is not defined"]}],"source":["%matplotlib notebook\n","deeplabcut.evaluate_network(path_config_file,plotting=True)\n","\n","# Here you want to see a low pixel error! Of course, it can only be as good as the labeler, \n","#so be sure your labels are good! (And you have trained enough ;)"]},{"cell_type":"markdown","metadata":{"id":"BaLBl3TQtrfB"},"source":["## There is an optional refinement step you can do outside of Colab:\n","- if your pixel errors are not low enough, please check out the protocol guide on how to refine your network!\n","- You will need to adjust the labels **outside of Colab!** We recommend coming back to train and analyze videos... \n","- Please see the repo and protocol instructions on how to refine your data!"]},{"cell_type":"markdown","metadata":{"id":"OVFLSKKfoEJk"},"source":["## Start Analyzing videos: \n","This function analyzes the new video. The user can choose the best model from the evaluation results and specify the correct snapshot index for the variable **snapshotindex** in the **config.yaml** file. Otherwise, by default the most recent snapshot is used to analyse the video.\n","\n","The results are stored in hd5 file in the same directory where the video resides. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y_LZiS_0oEJl"},"outputs":[],"source":["deeplabcut.analyze_videos(path_config_file,videofile_path, videotype=VideoType,save_as_csv=True)"]},{"cell_type":"markdown","metadata":{"id":"8GTiuJESoEKH"},"source":["## Plot the trajectories of the analyzed videos:\n","This function plots the trajectories of all the body parts across the entire video. Each body part is identified by a unique color."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gX21zZbXoEKJ"},"outputs":[],"source":["deeplabcut.plot_trajectories(path_config_file,videofile_path, videotype=VideoType)"]},{"cell_type":"markdown","metadata":{"id":"pqaCw15v8EmB"},"source":["Now you can look at the plot-poses file and check the \"plot-likelihood.png\" might want to change the \"p-cutoff\" in the config.yaml file so that you have only high confidnece points plotted in the video. i.e. ~0.8 or 0.9. The current default is 0.4. "]},{"cell_type":"markdown","metadata":{"id":"pCrUvQIvoEKD"},"source":["## Create labeled video:\n","This funtion is for visualiztion purpose and can be used to create a video in .mp4 format with labels predicted by the network. This video is saved in the same directory where the original video resides. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6aDF7Q7KoEKE"},"outputs":[],"source":["deeplabcut.create_labeled_video(path_config_file,videofile_path, videotype=VideoType)"]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import pandas as pd\n","import numpy as np\n","\n","#plotting confidence of left wrist over 1048 frames\n","df = pd.read_csv('/content/video5_5000_iter.csv')"],"metadata":{"id":"V9wigfML5io5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"id":"fC4gTGam8Eng","colab":{"base_uri":"https://localhost:8080/","height":300},"executionInfo":{"status":"ok","timestamp":1650196533005,"user_tz":240,"elapsed":404,"user":{"displayName":"Raima Sen","userId":"00263744560502469226"}},"outputId":"62369581-1ead-432f-daad-6022e92f3b93"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["      scorer DLC_resnet50_Video5Apr16shuffle1_5000  \\\n","0  bodyparts                          p_left_wrist   \n","1     coords                                     x   \n","2          0                   -1.4365110397338867   \n","3          1                   -1.4365110397338867   \n","4          2                   -1.4365110397338867   \n","\n","  DLC_resnet50_Video5Apr16shuffle1_5000.1  \\\n","0                            p_left_wrist   \n","1                                       y   \n","2                      7.5527849197387695   \n","3                      7.5527849197387695   \n","4                      7.5527849197387695   \n","\n","  DLC_resnet50_Video5Apr16shuffle1_5000.2  \\\n","0                            p_left_wrist   \n","1                              likelihood   \n","2                    0.005238324403762817   \n","3                    0.005238324403762817   \n","4                    0.005238324403762817   \n","\n","  DLC_resnet50_Video5Apr16shuffle1_5000.3  \\\n","0                           p_right_wrist   \n","1                                       x   \n","2                     -14.569314956665039   \n","3                     -14.569314956665039   \n","4                     -14.569314956665039   \n","\n","  DLC_resnet50_Video5Apr16shuffle1_5000.4  \\\n","0                           p_right_wrist   \n","1                                       y   \n","2                     -30.200977325439453   \n","3                     -30.200977325439453   \n","4                     -30.200977325439453   \n","\n","  DLC_resnet50_Video5Apr16shuffle1_5000.5  \\\n","0                           p_right_wrist   \n","1                              likelihood   \n","2                   0.0014511644840240479   \n","3                   0.0014511644840240479   \n","4                   0.0014511644840240479   \n","\n","  DLC_resnet50_Video5Apr16shuffle1_5000.6  \\\n","0                            p_left_elbow   \n","1                                       x   \n","2                      -6.179771423339844   \n","3                      -6.179771423339844   \n","4                      -6.179771423339844   \n","\n","  DLC_resnet50_Video5Apr16shuffle1_5000.7  \\\n","0                            p_left_elbow   \n","1                                       y   \n","2                      1107.9605712890625   \n","3                      1107.9605712890625   \n","4                      1107.9605712890625   \n","\n","  DLC_resnet50_Video5Apr16shuffle1_5000.8  ...  \\\n","0                            p_left_elbow  ...   \n","1                              likelihood  ...   \n","2                   7.987022399902344e-06  ...   \n","3                   7.987022399902344e-06  ...   \n","4                   7.987022399902344e-06  ...   \n","\n","  DLC_resnet50_Video5Apr16shuffle1_5000.38  \\\n","0                          t_left_shoulder   \n","1                               likelihood   \n","2                    0.0004296600818634033   \n","3                    0.0004296600818634033   \n","4                    0.0004296600818634033   \n","\n","  DLC_resnet50_Video5Apr16shuffle1_5000.39  \\\n","0                         t_right_shoulder   \n","1                                        x   \n","2                          6.4664626121521   \n","3                          6.4664626121521   \n","4                          6.4664626121521   \n","\n","  DLC_resnet50_Video5Apr16shuffle1_5000.40  \\\n","0                         t_right_shoulder   \n","1                                        y   \n","2                       29.631874084472656   \n","3                       29.631874084472656   \n","4                       29.631874084472656   \n","\n","  DLC_resnet50_Video5Apr16shuffle1_5000.41  \\\n","0                         t_right_shoulder   \n","1                               likelihood   \n","2                   4.3004751205444336e-05   \n","3                   4.3004751205444336e-05   \n","4                   4.3004751205444336e-05   \n","\n","  DLC_resnet50_Video5Apr16shuffle1_5000.42  \\\n","0                                   t_neck   \n","1                                        x   \n","2                       17.854537963867188   \n","3                       17.854537963867188   \n","4                       17.854537963867188   \n","\n","  DLC_resnet50_Video5Apr16shuffle1_5000.43  \\\n","0                                   t_neck   \n","1                                        y   \n","2                      -10.152593612670898   \n","3                      -10.152593612670898   \n","4                      -10.152593612670898   \n","\n","  DLC_resnet50_Video5Apr16shuffle1_5000.44  \\\n","0                                   t_neck   \n","1                               likelihood   \n","2                    0.0002620518207550049   \n","3                    0.0002620518207550049   \n","4                    0.0002620518207550049   \n","\n","  DLC_resnet50_Video5Apr16shuffle1_5000.45  \\\n","0                                  t_waist   \n","1                                        x   \n","2                       19.504091262817383   \n","3                       19.504091262817383   \n","4                       19.504091262817383   \n","\n","  DLC_resnet50_Video5Apr16shuffle1_5000.46  \\\n","0                                  t_waist   \n","1                                        y   \n","2                      -13.459928512573242   \n","3                      -13.459928512573242   \n","4                      -13.459928512573242   \n","\n","  DLC_resnet50_Video5Apr16shuffle1_5000.47  \n","0                                  t_waist  \n","1                               likelihood  \n","2                    0.0002358555793762207  \n","3                    0.0002358555793762207  \n","4                    0.0002358555793762207  \n","\n","[5 rows x 49 columns]"],"text/html":["\n","  <div id=\"df-3c272cd6-679d-406b-b190-9409c7cfc99d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>scorer</th>\n","      <th>DLC_resnet50_Video5Apr16shuffle1_5000</th>\n","      <th>DLC_resnet50_Video5Apr16shuffle1_5000.1</th>\n","      <th>DLC_resnet50_Video5Apr16shuffle1_5000.2</th>\n","      <th>DLC_resnet50_Video5Apr16shuffle1_5000.3</th>\n","      <th>DLC_resnet50_Video5Apr16shuffle1_5000.4</th>\n","      <th>DLC_resnet50_Video5Apr16shuffle1_5000.5</th>\n","      <th>DLC_resnet50_Video5Apr16shuffle1_5000.6</th>\n","      <th>DLC_resnet50_Video5Apr16shuffle1_5000.7</th>\n","      <th>DLC_resnet50_Video5Apr16shuffle1_5000.8</th>\n","      <th>...</th>\n","      <th>DLC_resnet50_Video5Apr16shuffle1_5000.38</th>\n","      <th>DLC_resnet50_Video5Apr16shuffle1_5000.39</th>\n","      <th>DLC_resnet50_Video5Apr16shuffle1_5000.40</th>\n","      <th>DLC_resnet50_Video5Apr16shuffle1_5000.41</th>\n","      <th>DLC_resnet50_Video5Apr16shuffle1_5000.42</th>\n","      <th>DLC_resnet50_Video5Apr16shuffle1_5000.43</th>\n","      <th>DLC_resnet50_Video5Apr16shuffle1_5000.44</th>\n","      <th>DLC_resnet50_Video5Apr16shuffle1_5000.45</th>\n","      <th>DLC_resnet50_Video5Apr16shuffle1_5000.46</th>\n","      <th>DLC_resnet50_Video5Apr16shuffle1_5000.47</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>bodyparts</td>\n","      <td>p_left_wrist</td>\n","      <td>p_left_wrist</td>\n","      <td>p_left_wrist</td>\n","      <td>p_right_wrist</td>\n","      <td>p_right_wrist</td>\n","      <td>p_right_wrist</td>\n","      <td>p_left_elbow</td>\n","      <td>p_left_elbow</td>\n","      <td>p_left_elbow</td>\n","      <td>...</td>\n","      <td>t_left_shoulder</td>\n","      <td>t_right_shoulder</td>\n","      <td>t_right_shoulder</td>\n","      <td>t_right_shoulder</td>\n","      <td>t_neck</td>\n","      <td>t_neck</td>\n","      <td>t_neck</td>\n","      <td>t_waist</td>\n","      <td>t_waist</td>\n","      <td>t_waist</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>coords</td>\n","      <td>x</td>\n","      <td>y</td>\n","      <td>likelihood</td>\n","      <td>x</td>\n","      <td>y</td>\n","      <td>likelihood</td>\n","      <td>x</td>\n","      <td>y</td>\n","      <td>likelihood</td>\n","      <td>...</td>\n","      <td>likelihood</td>\n","      <td>x</td>\n","      <td>y</td>\n","      <td>likelihood</td>\n","      <td>x</td>\n","      <td>y</td>\n","      <td>likelihood</td>\n","      <td>x</td>\n","      <td>y</td>\n","      <td>likelihood</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>-1.4365110397338867</td>\n","      <td>7.5527849197387695</td>\n","      <td>0.005238324403762817</td>\n","      <td>-14.569314956665039</td>\n","      <td>-30.200977325439453</td>\n","      <td>0.0014511644840240479</td>\n","      <td>-6.179771423339844</td>\n","      <td>1107.9605712890625</td>\n","      <td>7.987022399902344e-06</td>\n","      <td>...</td>\n","      <td>0.0004296600818634033</td>\n","      <td>6.4664626121521</td>\n","      <td>29.631874084472656</td>\n","      <td>4.3004751205444336e-05</td>\n","      <td>17.854537963867188</td>\n","      <td>-10.152593612670898</td>\n","      <td>0.0002620518207550049</td>\n","      <td>19.504091262817383</td>\n","      <td>-13.459928512573242</td>\n","      <td>0.0002358555793762207</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>-1.4365110397338867</td>\n","      <td>7.5527849197387695</td>\n","      <td>0.005238324403762817</td>\n","      <td>-14.569314956665039</td>\n","      <td>-30.200977325439453</td>\n","      <td>0.0014511644840240479</td>\n","      <td>-6.179771423339844</td>\n","      <td>1107.9605712890625</td>\n","      <td>7.987022399902344e-06</td>\n","      <td>...</td>\n","      <td>0.0004296600818634033</td>\n","      <td>6.4664626121521</td>\n","      <td>29.631874084472656</td>\n","      <td>4.3004751205444336e-05</td>\n","      <td>17.854537963867188</td>\n","      <td>-10.152593612670898</td>\n","      <td>0.0002620518207550049</td>\n","      <td>19.504091262817383</td>\n","      <td>-13.459928512573242</td>\n","      <td>0.0002358555793762207</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2</td>\n","      <td>-1.4365110397338867</td>\n","      <td>7.5527849197387695</td>\n","      <td>0.005238324403762817</td>\n","      <td>-14.569314956665039</td>\n","      <td>-30.200977325439453</td>\n","      <td>0.0014511644840240479</td>\n","      <td>-6.179771423339844</td>\n","      <td>1107.9605712890625</td>\n","      <td>7.987022399902344e-06</td>\n","      <td>...</td>\n","      <td>0.0004296600818634033</td>\n","      <td>6.4664626121521</td>\n","      <td>29.631874084472656</td>\n","      <td>4.3004751205444336e-05</td>\n","      <td>17.854537963867188</td>\n","      <td>-10.152593612670898</td>\n","      <td>0.0002620518207550049</td>\n","      <td>19.504091262817383</td>\n","      <td>-13.459928512573242</td>\n","      <td>0.0002358555793762207</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 49 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3c272cd6-679d-406b-b190-9409c7cfc99d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-3c272cd6-679d-406b-b190-9409c7cfc99d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-3c272cd6-679d-406b-b190-9409c7cfc99d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["array1 = df['DLC_resnet50_Video5Apr16shuffle1_5000.2'][2:].values\n","array2 = df['DLC_resnet50_Video5Apr16shuffle1_5000.5'][2:].values\n","array3 = df['DLC_resnet50_Video5Apr16shuffle1_5000.8'][2:].values"],"metadata":{"id":"iO9Q_9Qb83WT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["convertedArray1 = array1.astype(np.float)\n","convertedArray2 = array2.astype(np.float)\n","convertedArray3 = array3.astype(np.float)"],"metadata":{"id":"ATGNWySf_V1f","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650196613076,"user_tz":240,"elapsed":11,"user":{"displayName":"Raima Sen","userId":"00263744560502469226"}},"outputId":"bad315e4-1cb1-4a8f-941c-36b2d4202da9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  \"\"\"Entry point for launching an IPython kernel.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  \n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  This is separate from the ipykernel package so we can avoid doing imports until\n"]}]},{"cell_type":"code","source":["plt.plot(convertedArray1)\n","#plt.plot(convertedArray2)\n","#plt.plot(convertedArray3)"],"metadata":{"id":"x6CLzYJoB946","colab":{"base_uri":"https://localhost:8080/","height":282},"executionInfo":{"status":"ok","timestamp":1650196615761,"user_tz":240,"elapsed":443,"user":{"displayName":"Raima Sen","userId":"00263744560502469226"}},"outputId":"d661b622-01a2-4d22-941a-bbe9a8e07916"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7f85ee243b90>]"]},"metadata":{},"execution_count":5},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2dd9gVxfXHv+fet9B7EQEFBEUUFUWsYERRlBii0YiJib0kmsQ0fyQaNUUlmoRoYqImarCjRiMKqCAoFpCOdHjBV6r0Dm+98/tjd++dnZ3dnd27t8/ned7nvXfL7Llbzp45c+YcYoxBo9FoNMVLLNcCaDQajSazaEWv0Wg0RY5W9BqNRlPkaEWv0Wg0RY5W9BqNRlPklOVaAJEOHTqwHj165FoMjUajKSjmzZu3nTHWUbYu7xR9jx49MHfu3FyLodFoNAUFEX3ptk67bjQajabI0Ypeo9Foihyt6DUajabI0Ypeo9Foihyt6DUajabI0Ypeo9Foihyt6DUajabI0YpeownItBVbsGn3oVyLodEooxW9RhOQ6/8zF1//28e5FkOjUUYreo0mBDsP1OVaBI1GGa3oNRqNpsjRil6j0WiKHK3oNRqNpsjRil6j0WiKHK3oNRpNybBjfy2em+Wazbdo0YpeUxJs21eLhsZErsXQ5Jg7xi/Eb/63BCu/2pdrUbKKVvSaoudgXQNOvX8q7pmwNNeiaHLMroNGWGxdQ2m99LWi1xQ9h+oaAQCTF2/OsSSaXEMgAECCsRxLkl20otcUPfGY9XDnWBBNzjFvBa3oNZpipdQebo0TotJ86WtFryl6LP2u9XxpsutAHWobDPedqecBlNbNoBW9puixLPnGUjPjNACAAb+fghvHzQUALFi3GwDwzpKvcilS1tGKXlP0WPpdu25KD+vl/tHq7bblSzbuzYU4OUMrek3Rw0wFr/V86eEWL18WJ+nyYkUrek3RE6VFz/TboqCwrnmnlpUAgIFHtgUADO3bKWcy5QKt6DVFT9JHr5V0ycGSL3njf48OzQEAzSvKciRRbtCKXiPl3jeXoP+97+ZajEiw1LvW86UHg+W2M/7HzbCbUnvpl9ZrTaPMuJnFk/gpEWG0TYnph4LHul47zIpgMXPGVKlFYGmLXlP0aOVcuojjMtbM2FIba1FS9EQ0nIhWElEVEY2WrB9CRPOJqIGILhfWXUNEq82/a6ISXKNRJegg7KTFm9Fj9ERs21frWFda6qHwEa9Xmbbo5RBRHMBjAC4C0A/AVUTUT9hsHYBrAbwo7NsOwL0ATgMwCMC9RNQ2fbE1pcSuA3V4Zc760PsHVfTPzqwGAKzZtj/0MTX5AX/pV23Zl3LdlJaeV7LoBwGoYoytZYzVAXgZwEh+A8ZYNWPscwBi7s8LAUxhjO1kjO0CMAXA8Ajk1pQQP355Ae787+eo2hpO8VrGGymGTicS1n+nNii1Ln+h8caCDTj67snJNMT89dq6txYxK9eNtugddAXAm1MbzGUqKO1LRDcT0Vwimrtt2zbFpjWlguVCCZtDPKhynl29EwDw9CdfhDqeJnc89M5K1DUksHVfDQCn6yaVyVQr+qzDGHuSMTaQMTawY8eOuRZHk6ewkB7ypEWvuH3LJkYw2lEdW0hk0OQzFWWGSmtodM6GJkr16kotvFJF0W8E0J373s1cpkI6+2o0AFKpZcMS1Ho7u3cHAMCJ3dukdVxN7rCuOH/tSyvpgR0VRT8HQB8i6klEFQBGAZig2P67AC4gorbmIOwF5jKNJmsEVfTW9jGJZigxQ7DgSM2EleQ3IiTfAFRiat9X0TPGGgDcDkNBLwfwCmNsKRH9joi+AQBEdCoRbQBwBYAniGipue9OAL+H8bKYA+B35jKND+t3HsTHQsY9TTiCKmetzAsXy71nDbby7j5euafZSSw4lGbGMsYmAZgkLLuH+zwHhltGtu/TAJ5OQ8aS5JyHpyPBgOoxI3IqB2MsbddJrgmquC2fvmy/sOMEmuwg5rbRL22DvBiM1TjJl+ivYnhQrG68+gvLtAqL4LeXKtaEKHEwtlQvqVb0Gk+K4cEI7qM3/sus92J48ZUCSR+9zXUD6edSQCc103hixKDnx2MRVsmqhleOenIm+h7WKhl3ry36wsO6RyyLPmGz6KlkJ7xpi17jST4oO5mCZozhlbnrcaC2wXd/1Yd71tqd+M+n1ZhTvSvQfpr8YddBI0vluJnVAOzXkPfcFfiwU2C0otd4kg+DjzIJPvtiJ+587XP89q2lvvsHTYGw33x5yPR8qRWVLjQO1jUCAF6fb0zXES9hqb67taLXeJJPD8bX//YxXp69DgCSlvz2/XW++4Wd7i57yd0xfmGotjSZZ8+hescym0XPLddx9BpNnsE/knf/b0ng/cMq+kS41DqaHDF58WbHMh11Y6AVvcaTfLLoeZjiACu/beBjhNtNkyMqy53qzO3aax+9RsORDz56HusBZcJ3L1IvhWBPd6llOCx0yuMSRc99zofAglyhFb3Gk9r63PsvZMo8SERMaIVdwoqhEIlxN8ppPdsBsF97xvK3h5pptKLXOOCV6PSVW3MoiYHNz+qwyv2t9EQQP49sP01BwCehG2Qqev4SlvL11Ipe44CPXghb7COf0AZ9qZDS9KmasKmrqBW9RsPBK/d8KNAgdd14rBNpMB/6eMARuFJWDIUIf3ktPW/zy7P8G3PKFlrRaxzwD0csD8ITlm7a61gWxBvT0Gi8uGT55b3Qer6w4C+vLB99QvvoNZoUvBVfWZavt4j6E2tZ9LGAmr5EdULBwmcnleWjL1VrHtCKXiMhwZn0h7VukkNJoqHBnPkUD6roS9X846jauh/XPTMbNfWNuRbFl5iP64b/XGqXVit6jQObbzrfHggrjj5A/hqrUHRQH32pKQMZ901Yiukrt2FOdf4XhrP76C3XDR9eWboXVCt6jQM3Kyjb7K2p56InDKxnOTkYC8ILn32JMx5837WdsK4bPRibohBOBe+6kYUFG3H0BfBDMoDOR69xwCvXXPk1a+obccJ97+H7ZxwpXT99hfEgEwF3veGd/yZs1E2h64TP1u5Az47N0allePdbHozFK8OL+uWOg1i7bb/tZW2bPJV3XdXMoi16jQNmezhyI4OlnF+bt0G6/lWX5dK2GsP56Avdor/yyVn4+qMf51qMrCGWimxMMNvLusAvZ1poRZ/n5KKr2ZgHfk1LJ9dGMGGrMem6SbupgmPrvtpci5A1xNd4PEaOmbGlqutL8NYvLHKhZ/n0vLl6MMSScBaiK0Fl5m59yMHYQrfooySKF262STAmuGtSlNql1Yo+z4nqfty4+xCqtu5X2jaRBxa9Kg0KvqVG880VOI4+v396Vli9xbhnVCp55RrxcjUkmKDcGRat351NkfIGPRib50RVnPusMdMAANVjRvhuK2b8ywWqh1Ux0sNb9IE2L0p2HzIqeG3YdSjHkvgjGiUNjcx2IzEGLNqwJ8tS5Qfaos9zcqFrwoZXbt9fi/rGaLr4qj0JFdXdmJwlGVAGhT321zbgmLsne2b5rGtI4I0FG/K+d1ToyM4usyU1y54s+YZW9HmILbwx4pvz9fn+0Sr246sJUN+YwMA/TMWdr30eWjYVxDTFYqSFjHrTdeP1W2TrVH76qi37UNuQwF+nrnbd5pH3V+Gn4xfhvWVb/BvMMwrq3STIWtvQKBgtcn99KaAVfR7yj+lVyc9Rxwn8+o3FvtuECa+0Xg4TJXU7w+B2WFGv81/dFHljozPBlYjsdy7esCeSnsXWvUbky56DzuLVmugQn5X/++9irN22n1tfumhFn4es3X4g+Tlqi0qlPXuki6KiE6esponq71Zxux+oM/K0eDUpi7B5Z+lXGD9nvWfbfnI+O7Man67Z4XoMTXSIxdyrtu7Hvz76Ivm9lF1nSoqeiIYT0UoiqiKi0ZL1lUQ03lz/GRH1MJeXE9E4IlpMRMuJ6FfRil+cZHIyosqtHsZHb7lUouqBrNtxMPA+bs/xS7PXAfBWtG7rlm92pkiW4fbCuefNpdi4O/8HMosBvzuvlCdP+Sp6IooDeAzARQD6AbiKiPoJm90AYBdjrDeAsQD+aC6/AkAlY6w/gFMA3GK9BDQecEojeovev8F0om6ikPcvU1bhkr+rzegMcjx+23019Tjm7sn4wBxEdWvHv/CKugAlpluyjt+9Xco9KhWLfhCAKsbYWsZYHYCXAYwUthkJYJz5+TUA55ExSsYANCeiMgBNAdQBUDORShh+wDEKC5m3SlXu9XRygkTxKD36vvvAZjrH5n/X6q37UduQwFhzENVNCXgFETGWmmKv0gvLpp4pRTdFEIu+1FBR9F0B8I7KDeYy6TaMsQYAewC0h6H0DwDYDGAdgD8xxhz5TonoZiKaS0Rzt23bFvhHFBsUsUW/WnGilEUY1431Qsi0gnGUBg/g5+JFsypnWfK6WvSi49elPZXon2xOwC9Fpeb3m3VSs8wxCEAjgMMB9ATwcyLqJW7EGHuSMTaQMTawY8eOGRYpf9m0+1BGFKUtMkVh+4QQXjnx8834cJXxAm5MMNv6JRvtkSm5fHz8zh2/3pokaz38oSz6YOJl16LPs3ayg9/1z5IYeYiKot8IoDv3vZu5TLqN6aZpDWAHgO8AeIcxVs8Y2wrgEwAD0xW6GJm/bhfOHDMNr87dEFgx+yEryOCF6KO/7cX5uObp2QCAo349Cd/8xycAjFTBX//bx7bIFL75XQfqcMaD72PJxszNRgzy8PK9k+TgMXOuE7np2bnJmcX29lJ2oZLrRk3MSIjKYCigLMW+90KpWfE8Kop+DoA+RNSTiCoAjAIwQdhmAoBrzM+XA5jGjDttHYChAEBEzQGcDmBFFIIXG1VmTpHZ1TsF1036Nydf4DtoeKXs4fjcnEb+5Q4jDHT55r3Sdj+u2o7Ne2rwzw/WBJTYHdFFEiQQlP8t1iCrlbrY6zxPWbZFGjljFLKw5PI5uLVDlojqSIWUj97Pzciv/8f06O7JQsBX0Zs+99sBvAtgOYBXGGNLieh3RPQNc7OnALQnoioAPwNghWA+BqAFES2F8cJ4hjGW2amThYpQIs8iEos+4PY2Hz3nuuAnnwBAPG7cPm6JxayeQSaVRZCm7TVDmW3/MNPjMzVQvW7HQSxYtyu4QPyxsjyfIR/wux78b9lf21BSCc6UkpoxxiYBmCQsu4f7XAMjlFLcb79sucZJar4RA3Hv3ygeNJWBQgBYtmkv3l++BV3aNE0u23WwLvl5x4E62/ZlpjVs1WQVsWSPRajpvVpyO1f9urTCMqHXkfxoyhYm9C7oLtv31+FQXSOaVsQ9txvy8HQAagno3ChFN0WQwVgAePzDNfjn1adkUKL8QWevzBOSypAJFnAkil5tu4sf/cix7A8Tlyc/JwSz13J7jJ+7HtMkSb0SgntEBbeKUulgSS1zz6Qs+nAnOtUz8P+Nj76/GlOXbcGknwwOdawgFJIlHhVBZzys3xV8Ul6holMg5AkkRIFYRGGZRWVPi+4Nvt1tkkpG1vZBDPpfvLookEx2H71P1936zxg+Xr3dvk7hNIsvCsb4noH//gCwTHGmrSY4KlFXLStTtu2SjaVzLbSizxN4RckrxjnV6flqgehcJ0GtXmv7KF03QahvTOC+CUuTg8aWPK/O24C/TFll21blp63babcA/c5HNZezKNuUokXvRyJRig4tA63o84QHJxvBSPYhQuCpj9em3XZUejawordqtWZpMFYU78OV2/CfT6tx0EpqZq6X9z5cxhk41bBYCBNlkmPyfLW3xn1lhuHl3l/bkEY7hYN1LW4a3FO+HqWbBkEr+jwjIfjoPSZmZh3RdfNLn9zz1vaRWvRCU16P7Z5D9rTAXj0M9wlTjNtGODZL1SQtk7zNcqlT+GPf878luRMki6Qmv8nXL9skDwMuBbSizzMSCWbTZVFYIJG5bgLGIFqyv7P0q0iODzhd4Z7FRHwXpF6q7ikQOEUv/H6GVGipbMA5l/lm+CNvF6KlglAIYfSL1u/Gg5OXc5Pf5Of91XkbSjIaCdCKPu9oSCTwwmfrkt8j6WrmyHVzsM5wGezOYMGNnR5ti4rWmiQle9jVLHrnYGzKPSVR9K6SZR7+txeCsk6Hy/75KZ74cC3qzHwVXhFQ2qLX5AViTHo+1bkMKssDk6KfBN2YYNhXk1Lu/KQXv8lmlqKWxf27/bYGD9cNWGq9zHXz9ueb5I1mAV7UdDp0Xdsacyq6cnMr8g2r6PsBcyyiXfNy121rG+y+0DnVjhyLRYlW9HlGnZBFK4ruv/ic3/XGYvQYPTFwO0Fz1nRoURH4GH4cqGtE//vek66bslyoySqcupr6BLbtq0VtQ6NjX9Fat5Sjl+smwVgyu2X1DmeEzUuzvatTZRJbZs002hnSx0gyeN1ZPdKSx415X+7Er15fHOo+/++8DZi6bAvK4sYvtOZgXHnqEcptXPH4zMDHLUS0os8zauvtij4Ki75esGB511AQHgmYJ/7i/l1CHScsVVv22b7LXDSn3j8Vm3c7o2FERWNZ6PwLwFIoqfZTrNmWu1BKKXnUE/TirjeW4KXZ67D3UPDIoJ+/ugg3PjsX5WYqjhVfGde/okyrNRF9RvKMvTV2n3NjBJr+NzmKurAewGzh8Ky4nLrXF6SSr1qq+0uhdGEq2VlqWZfWdvcFX3gk3+BfcqopMHKBdd7TGYsS77MgM7FLBa3o8wzLKrGIYjA2qnjukScdHmh7y687rF/nSI7vR5iEcPPX7UZtQyNuGDfXtrw8FnO04Zy1nL+Gc1Sum2xFDqVzn1cIPS2t551oRZ/n5JPFeHjAATnr4W3ZJDsplURXjaryuO2F+Y5l8bgzffHMNTvsx8ujayMS1WBsKm1EOtIAs9buwKgnZ6LBpZJLOh3XMsGiFyOgtIWvFX3g2PBsk08z+VRFWSe4QVTN3iDWY4/2zZS3FakQFMPU5c6EbLIomr9Pr7J9Z8hj141NsPQVXbrx53e8vBCz1u7EVmFWcmoeQ/j2xdQU4ovtnSwkkct3SkLRM8bw5/dWOvKpL9m4B71+PQkzVuVvndr8UvRqslhpdi3ZVX9Buj/V4bpxaU8cVJVuY7luPGRiLNz1yYZxwR9BfGcxxrBjvzMNhLQds6GX50QTQeTWu2iM8D6PEdms+LbNo4/+KjSKWtE/+v5qzFyzA1/trcHfplXhmmdm29YvNGOwo5y5GTX51OEIKorfTMV02pcpBudgrLxFt0IpPHFJ1I3jeCEVvcrx/Vi1ZR92esx4tfnoBeX674++wCl/mIr1O/3T9Fq/b22aUUVuPYJ0Cr+4QQCGH3cYAODUHm3RoUVldI0XKEWt6P8yZRWu+tes5Pc6YbJEpRmGVVPvjKvONHUNieSLBgCuGiSP/c3kYFjQtp+cESzBWlD/bhClqZIDyK01Nz+xTBav9BEJxmxuoLnVO/Hmwo04VNeIBycvd90vikiqC8bOwPC/znBd7+VqmbHa6MG+/flm6frfv70sOZEo6rvPbdZqlL2cGFHy5dapVZPI2i1kilrRW4iFoC2shzgX3pE/TFyGbz72CdaY7qQm5fJLkUnRMt1bCOq6CaLoZcpS1XWj8ruPaGeMAXhVg0owZrOWL398Jn7y8kIMfmg6nvjQ/aXYEFGmOtHfbcMWdWNXrtbL6Y/vyGcuP/XxF8mJRJl+Ng6YmUU/iNB9SsRNYDPlv3lIr8jaL0SKVtHzFgJXvMmG6YbNiR/cmmW6y+x+u1o6acq2SVLUWqXtKCIVrOZVeg5f7jiAqcucg6JuyEJGRSs2nd7QX0edBADo3tZ90JcxuSLc7uP/jsKid2PH/lqs2rIvdNSN07KORla/S3GoLnwqZREiShYVWb11n3n8PPKB5oCiLSX48HsrfbexLPpc+sH9Dp3u/fkDSeighZeiP6xVE2z0eEmsEQa2ZbAAFv3QP3+YvgIMEUfvRrOKMrMN91YaQxayiMJH78awsTOw80AdZv3qvOQyUdF7KX5Rtqj1o3jsXh2bY+22Azi2S6toD2QSN6050W1bahStRT95sdP/KN60RM5Y6Wyh6jZKR7QDPpXuvdqO+dwZn1Rt994AwSz6TFi56Zw7v/TFgPGiDNPjUv2tsgIpflgDtLaZsY7eorumf2m2PT1Gpnu7lWWGa8ytwHy6WGGyN59zVEbaLxSKVtHzpO5V+80UU3iYg/Dhqm3oMXqiM45cgmjZuNmG6TxoP/Sw5v2IpzltfuH63ZFNtlFFNnM1LCpFwxMurhs/VC36dIpXf8TXxFW4lNNXbMVj06tw74SlAKJ/NtywRKtXGCDnkRlyMiwXZNc2TXGYz8DsnkP1+NFLCxwFa4qBklD0yUFBl8HYqKyW/5rZ8+av86/zallZfgNz6Yg278vw9WbTLVbyzcc+4Sr+5MY3lk5PjRTujXGfVodz3SgqtXSuwJ1c9S+Vdq77zxw8/G7K3ZnM9ZOGDDxu7VjF0sX0wX5MVFT0/MQ3r4F1ALj7f0vw1qJNeOaTLwLJUgiUlKIXUbHawqCiIw+Yg0/f+ddnALwiRNJQVj7rPZuOYNZ4ynWTfltBjhcFKvHdz836MtTLRNWijyoZmdiOSrPWPlbq33SxThN/aL7n+6OXFgRqb2+N2uBtjFP0lZKslt8e2C35+a1FuasfkGlKQtEnFY6w/N8fG2/uXAzGilkq3UhLNJ8HWnQX9e7UwrarV+DN36dVua9Mtm+QrfMrHiatl6Ti1Pwwh/ByU2RjvIg/xHMzq6XbpOu6U0FWF0AV1fPE/w6ZRZ/tDKu5oih/JWMM1Zy1kHLd2G8Oy7WRy8FYP9JyPwTcvmeH5ql9iTwtSs8YbhOWZddNtBa96brx8SiEibupqfdS9IGb88Wrzu5v3lwq3Sf6RGDOCKyn03CRqN5TfLqLJmVORS8tAVmEkZhFqehFCzLhYtFb5OLCqiv6NI7h87B6TTAipO+9UU2BMO/LzJRzi+K6+inyMHOfDtW5W7L80aIyQMIY55lK+Mj/pEXrg1Us41E97/xzJpuUmMep+iNFSdET0XAiWklEVUQ0WrK+kojGm+s/I6Ie3LoTiGgmES0losVElPE5yeIAp9tgrEV9RL6FIK14WVk86VjDfkW5HVNjuGMRpT8gaylJv3DC1+dv9Fwf9HgW6VxW66f7tRHmEF4pNzLRuwxzFVV94EHhr1G5QnI5N1RDVPlbuJQrT/n+ciKKA3gMwEUA+gG4ioj6CZvdAGAXY6w3gLEA/mjuWwbgeQC3MsaOA/A1ABmPXRJvAubiurGozUGuG9XHOcrH/vxjOykfi0Bpm/QJRYs+KjeBo4cSwdnzkz2MYvZS9PytG5X/WHTBqUqsGh2kgmxg/pyjO4Zqa9hfPsRsn6LeQ8y2+V9eIXHdFKObRobKnTQIQBVjbC1jrA7AywBGCtuMBDDO/PwagPPIuLsuAPA5Y2wRADDGdjDGMq5V3Wb3uV3ToKFdUSAqCDfZgiR7euLDNegxeqKr8hEVqkMGh0WvfGgpSdeNz+mV9Rx+PLR3MgNhuscPgzhhqp1Lqtswx/jBC/Nx0GXKP/9ykkWJhME6u+t2HMT/Fmz0vO48CQYc07ll6nsEPV++hbA96dVb/Wdlf9Oshsa/5MQ6BIY8paHpVe6krgD4ZNQbzGXSbRhjDQD2AGgP4GgAjIjeJaL5RHRn+iL78/ysL23fk/eTyzXNhaJXTZ0aRJE8ONlIUiUWA7coE6a7OqNU7N/d8u+oknTdhLDoBxzRFgcD9rS8rOvTerYL1FZyMNZsU1aIBAivKD6t2iFdzv+EKFXQ7oN1GPLwdNwxfqFj3RsL5K6zBGO235eOG9Has4pT0v/8YI1ju9temI/r/zMncPvz7j7f9t3S73aL3nkN8ykNeCbJtNOqDMDZAL5r/r+UiM4TNyKim4loLhHN3bYt/Sx2D71jz3Pjl0WxLo0wL57l5uQPlfjnU03FY3Vf3Z6hIPehZbG4ha35Ds6K20dm0QdX9H6yqh0/ddyg4w2ij95t/6i7/jZFr9C2kuuIgE273esGV2+X55oXk7alUxzESs1wzdOzpestf/3ExZsxbYV6cjsAaFoeR3vBcLJe1Pxlkz2X2nWTYiOA7tz3buYy6TamX741gB0wrP8ZjLHtjLGDACYBOFk8AGPsScbYQMbYwI4dw/ntvHCzRPp3bQ0A6Hd460iOU6XQpRRlcktPLG6nghVK5pbASRz7ckbd8K4b7/BKFVTDK2VKNBbCdeQcXA62P484mc7tVGzaE67wult7QS1oFYv09fkbbccT91m22V6Q3qKR2fsrmVKKR7Zvhov7dwm9v8xQSP3e1Dq3Ppn/ksJHRdHPAdCHiHoSUQWAUQAmCNtMAHCN+flyANOY8ZS/C6A/ETUzXwDnAFgWjejq8Mm19tbU44kP1yCRYOjS2ggAijrCqlEh9suyclOhn/LbK8jDZW1rWU+D+3SwrY+Lmcr8wisjsuj98lXJjhOj4I4j8VzxFuh3T5cXdrE4+Yg2Lo0a/9xkmbVG7oIJC6+EVa69qiFw0SMfJT+LinHq8i2ubfMv/0zNh4gTRRIhdePZPR21hO0WvXPfl2avx71vLsEPX5iXXFaMKY190xQzxhqI6HYYSjsO4GnG2FIi+h2AuYyxCQCeAvAcEVUB2AnjZQDG2C4i+guMlwUDMIkxNjFDv8UV3nXz+7eW4dV5G3BEu2Z4b5lxg0edOVElWCHBvXy8COIDtrYdNnYGfnr+0Whabo8ycPMzy45F6QfdKP9GmVhRhHfyLqNWTco9t23d1L5ezHXj1rupj6iIiEVQxRpG+bZqopadPJGwFz/PVB59ovQGeq0rc/fX++Hur/cz2yTbOi/GzbSP6RWhnlfLR88YmwTD7cIvu4f7XAPgCpd9n4cRYpkzUgoHmGvOhn3yo1QFoKCZ8/yP53+nNHIW/bwvd7m6fcJY9AAwduoqXHhcZ9t60e/tLNSR+kyUfq4V1Th6me3Ol4NTRTzvfHii/0tOlEnepkj0906w7cMopTnVasnuEsx+XjI1cBmPUfT5pqz/vEUv3GcdW1ZKU0FHWag8XyiJGQRfbDeUKAPDF+bA04J1qTztUVsqKl0/a5v6xgS+9c9PMWutPC44kNxfhVMAACAASURBVKIXvos3thhdxhjw9ueb0GP0RKMqEbPvG9VgrN/5lR0nlmZ3HrCfj7KAMemOwViX3cPmUXeN9OKaE5VfY4I5CnqHuXe9CsqIbfMy8Pe1ahsqxIjSegZlBsmKr4zAiKWb9nLb2bcpd7nBo6xfmy+UhKL/6fhFnuujrvij0pxlNdjyhkvbCq/pReUkhlcO+P0U3P6ikTXwgrEzMHNtyt8ciUXvMyPZdjCBGAW/LuLW/Lnze2nVCvlnxKI06bqRRH74wnxbcXgLvpclnrd/TK/C4IemYwKXZTGTeYQYsyt6SxlPXrwZZ42Zhg8jqPN6ZPtmyZd6lL7x95YabtkNu1IvJPFcud3fuUqrnUlKQtFbuF2/6H300URLAMGUneiKES36QQFjydO26M3/fl1h2WGIKPCDL27Onzo/PX2Ii9l/+0dnJz+v3LLPVcZ0WSCpW8DLLCocy+34Yy6lb6YqMwFm1I1NHuP/wg3GC2rppvC5agCgZZMynHtMJ8Rixm+Ncj6LzKXW3CwPeemArhhzWX/XeyJib1xeUFqK3mW5l5/18w27ceO4OYGmg6soqEx0Dx2H5W7kc4/piM4+FXZ4DAWSauDeS8SsF+ry+P1WedSN/fuJ3V2iYuxHlAvAYUVaeXF811S47fqdhkU4apA8aif6OPpUg4+8vxrTV3rHlN/07NxoBeAQK2ht2n0INfWNaU+ks6hvTKCiLIY4EWobGvHdf38Wqh3Z/cNnYrW44/yjcfeIY/GnK07EqEFHuCp6bdEXOtz14yMPvCzwn45fiKnLtyZ9+24vBX65ig7PxM0ktsi7G4goUE4fMQVCmEfb6mH4/Va3tL38bn07t8SpPdoGOr7sOoR1wfTp1ALXntnDsTydKfQySXiZP1i5Ddc9k5olKjvS3DSqiPmREHz0Ix/7xPZiSfcWbmhkKI8TmleWYc+heqWKaDKjQZa87+YhRo1Y/p5pWhHHjYN7JcNL3SfBqf2wx6ZXhe7V/OyVhTj2N++E2jcMJaXo+YdyxAmpCRpe7hHejzdl2Rb0uWsylnEDPBZ97pqc/HyYgtWYie6heIOS8PlQAEXPmN1SCjNT1Tqtfq6bxz90ToUH7NerSXnMYUm2FMIExUhH2QvGr+i5G0TA6q3OiUVRDyJ6vTiyHd+dEFw3gDGmFMVwRSLB0JBgKI/H0KyiTNkFpRrOqnK7um2iekkffnclRjz6sdrGAq/P3xjoeUyXklL0PGFyijz1sRGSuXijcxCNp01T75ht4/iZf2jFBzKoNSv2CIKimtRMui/s16gsHnM8mX5Jv+wPrLcV5weBnBPOEL3rxutmzLZLQYy6iRJLYZfHY4jHgBVf2V+ibs+HWx4nN8K4mXR4ZYFwYjd5SgPXPCIq4ZBAMgRSjF4JQ10GTHpv14292o5KWxFkmwFghOLtORQsOzVjkigJyTbeR7ddZAAe3XUYuYL+dMWJ8sZIHo6XjlKQxXB7WZMRz83yJcEyFztvKezyOEmviXXctdv2YyX3EqjPQgLCIOHRhUJRKnq3S2ALFbPNQHRvS6YW/BSmyi3glo9GhltKW8dxhQOT8E02QOXGzgO1Nit+1trgU/15xbR9v3/pQRHx94gKQXwRiN/5r3UNxpcm5c6c5Ma2DMd3bYXLT+kmXU+QX/d0XDd/n+6su+vlusm2RS+mQLCwzkI6ys4KbiiPx6SK3jqvQ//8IS7864zkclUDKZ0zpXJNC0zPF6miZ/KReP7a8IM6KgNqfEY9v0IZKjdBEIt+697gShIAXufSzxIBXVo3Vd53y95a2zkMUyzaKyZcbX87ogh+TfLXuHMrI7vheX3lxVf8lCgRSXtyUT/wXjom27olweRPhnUdPnFJtayCNQBfURbDWkn2TLfrITOQzj0mXCJENx95roIpMklxKnowX7cDb9GrXLMxZq53wN/vp/LiCGLRRzVZJ+h4qk3RhxiM5c/rEy4Drm5UlsVsmo3gLEguRmCIZ51f3atjC3w6eihu/dpR0uMlEs5xiOMOb4U2zcqTx5dZ9EEe+NduPcN3Gy8rOdPugo/uPBdtm6XGl/x89DND9PIsrF5q84qyZHpvHrfDyqLePN2CHrftFhcDSuWaFpaaL1JFL3toAfvN8+ZCfnahe1uyajZ807sP1uGSvwUfea9vTKDvYS39N4S8dxIUIxtl+MHYMPDnlZ/NqcLhbZqiQ0t7VSe/NNDi8ym+cA9v09Q1502COY2DeIzQaPqSieRjM0EUfZc2/j0qr+Z27K9TPlYYurdrZnNbMCYPZ4wijv6gWSC9WYXdlfaT8/oAcB/7kA3Gyu4LS+4wk/5ULqm26PMAt4FEt4se1FLim5m4eDMWbxRiaVVcNw0J5QlMSlkMQ0xK8uPLHQf9N/LA5roJuG+75hV48LIT0KujMa7g54oTjwfIH9gg5yFGlIwOIZD0JaHigZv283Pw6q1nKKlHr0u9OWTu+yAc26VV8nNjIppCe6u2OMNSD9QaFn2zCnuIrFXAW7zn95ix8nxPuGNLwx3XQzL2VGMW3xEzuKqgZNFHpOfF3EWZoigV/fLNe6UXy839EPSieWXEAxQHYxuZcvFnlZCyD1Z5z6BUtcKaVwR/MFxxBr0EonXTclw5sLv7Bj5tSuPoPfKbiKsWrt+d9CWv+GpvaNdNr44tcGoPtfQTMtVqGSJBU1gE4Spz5u+T3x+YXCaLoweCGw3rJAaD5R9vWhFPKmwgVWmKCS9Q67x8VJXKr/Ps9YPw0/OPxt+vctQywilHtsMR7ZrhZ8OOCSYssjsYu04r+nAs3mBY17Jr5ea6iLpAsMpN0JhIoCxGePjyExS2Td/nf8s5vfyFgrsiDNNZ91OCG3YdxIVjZ3huc0Q7o5BEzw4tHOscrQsLEoyhddNyrPj98OQyt0Flv1O8dNNeqaEQJOpGdmjR2pWdsr2HDOs3SNRUUM452ihS07ppOUaahbUTLJo4enFCImMM15ozfpuWx/G/285KrrPcY27HtcqEvnjjaTi2Syv85Pw+OEIoNmL9jhl3nov+LqHWXmSiDsAvXl2EHqOdpThmRJAYToWiU/Tb9rt3b90e8uD3sj0+PQwNjQxlcVKKbW9IJPDVnpq0QvkGHKGYPiDC7F12g94p+7hPq5NJwyz+b3hf26zl4ccfhldvPQNXDXJa9qLLzeHKYYaFyIdUeuU38er1jOjfReoGSDCWtEL9kLUvDiTKFEhto2H9NigE0vfu5HwhynAWpUmpgitO6W7K4uL+8mi3oTGBuoYEfvTSAjw/60sAzpch/7VJeQxdubEL63kQffRiGxU+k+WCwk++a1Lm36sN+iS+Nm+DdPkUl+peUaNWaqaAkM1etHALp1q9dT/+NWMtbhpit3pl2QUBf+Wu0kOwpn+rDHgeqmvE6Y++j0sHdMXYK0/y3T5q+h7WMtQLgB/w5p/bWWt3SBXan6440RHHTkSubg+nYjeWPDBpOTq0qECCOXtxRITqMSMc1lX19gPoKBSY5undqQXO7N0eT8xYi8NaNcFXew2DojHB0KZZhXTyk4hK8jbZnWPpd5XxANls4ZZNyrCvxj4XwytU1ZLJirrp3KrSNUJFZNjYGcm8UG8t2oTPN+x2REvd8lwqX05TwVVoPQ+OXPzCd1W3pyp8NM85CuGaUQ3GZiIrqoyis+j9Kgm5cf+k5Y5ll/7jU+m2Yg4ZEZV7oKExgXjMHpv93dOOkMZ57zcHrt5YINZkjx7Zi6db26Zpx5Pxu496cha+86/PHOfpSEkX3LNNR5SNwZMz1uKBSSvAGFOOuki4zL2wiMcIzSrK8PoPz8TbPz4bXzOVAWPye04WUSW7L8TzLQsMaGQM4z6tRrUk3lyFmb86L+mOsRAl5vM3WXmNrFKCQeZffCHI+MrcDfhgpd09MXV5ajxJ7FlY50M8DWJnJky4rxeBK3uFnKCbSDDbuV6z7QB6jJ6IvTXBZo4HRSv6EPiFKarcM/UJo8vPGyZHd26J03o5rde9NSozY6P53bKftkuSHTAoKpFNfRTdDsk2fc509Y4DgUIBvXpX1qqTj2iLDi0qcdfFxwIwlLBsv6M6tsAd5/fBP76bGiiUjaM4Fb3z2LsO1OHeCUuV4tbdTvMjowbYvlv3sPW4nM0Vkk9Z1cY5djxTERZhEWcq870JHhW3VVSoGGphx/X+8UEVLn70I8fy1Vu8Q4fTpegUfZgsixbvLNmstJ3Nog95uMYEQ1nM7rqZskzur+MLTbjKFPH77ZITUxZgPGSlcN5Clz0WVggcAHRoUYE2zSokW7njsOiF77PW7gwUR+1n0du3Nb43NCZcx1nuOP9oXNw/Nd4g8yqK7crOkxWKKNteBcvNMOay/ilZzGa+d/qRqLr/IpxyZGoMxzI+GhlDgjknikV5q1mupuoxI1A9ZkSqNyFczK+E0NK2zYPdK0FQUeJhh8v+Ns2Z9gIAdoRIERKE4lP0Lnfhz4cd7bvvrc/Pj2T2oUob9ZbrhnuI1u08iH5dgkcJRIn14jnu8FQ8dSyGUK6bVk1Ssyxlp+T5WeuSn0Nlx/T5HrTd6u3uoW6i5c3XlJUN8suURbe2zfDApf1ty8RdZb7fT9ekLHnVgV8eS5nyxVNamRlW2zavcNTTtX7rNU/PRoIxpSR+W/fVoLYheNpd8fpYx/7ZK4ts7f3ytc9tL7yuCpPPMklYPeFWRevu/y1JRxxfik7Ruz3YccUHRKV0XxRx9FbRBV6BlMfJ1oUWlYKnTMpbepOqkZpaFiPCWb07uOzhTpAyiCqG6g9c0hdYyB6+IEWsvbZ1KHrus8zKdtMD5x9rH4OJxwjPfPIF5lTvdN3vkfdXJz+XS5Tu1r0pa1fcvXrMCFRKokiuPbMHfn1xX+k55X8PY2q9jkH3v4/bXvDvefphHWr2FzvxPufLP1DbgCdmrE27fRXUZsZGe8yaDOemLzpF7+ZndfPdf00YYfcqK2hxw7i5qYdL0uxfp672tW4aEwzxWMxmLYmW0zGKKRKixPLHkxBC+i2XrI5eNAbwq6pEH42fs972vb3Qfc/kpHQ3141snReOKCAQfvvWMlzx+EwA/tEc5ZKomkEPvK98/L9dZfjqrz79SNw85CjpS0D8PWIv4u1FfDRVKsPl1AhCBfn7gH8WGxIsUH4oFb548GKMPOlwfONE+0B1Jn30bmQqHbRFESp657LnbhjkeiLFF4DqzfT4h4Z18WnVdse6Ret34z+fVHvuX59IoDxOqGtMvRCaVdofuiDd9KCuj+oxIzC4j7uVzjcX1psVzKL3l39/rX1Q+pVbhCRhGXxYxPuK/y5T9FbqBr92eIVRU99oO9etJQVsKnzCCv1cCpeceDiqx4xwTdcMOH+PaIDw2SaNugGehwzEbLNnA9hnhBOiT+pGRHhk1AA8epV9oFrlKFGnuok6ikikCBW984S1a16BIX3ksbGiguTTB3dv5+4HtKyN/y2UJ+vy6hlYYWtlsRgOa5U6hhXJYRGkwEmY22TBOvdKWfx5CRszvHabPdTuXzPWYtcBeWIulZ/K+8KvOKUburdrltEHZNSpqUla4iA/3+MRZejcqhI/PV8+JuTIqc/dJn2FGqKdWjrj+v2uRRSZTsUxBy+3Z2NEs2ctPuMiixq4Z+ioji3SmjAYBJUXStRJzTIdLFh0il5GWSyGftzgIs/MNfaQtfpGhpr6RvQYPRHrd7r7bC3XjKvl5nHlrERZZXFCc86Kb9PMbsGFGXgLgmghu2FV1kqX+yctxy9fWyRdp6Kg+FP6sFkJqh+XhGviYrWoKVW+4nzfboOxgFPRn9W7g2OAM9mOw99tVxjWpL7+XVtLq12Jk49EeFm6KNQuVpFRVlnLojHBQitg2f39m6/3S37mrfuzenfAvz/+ItRxgpILiz5I7zcMJaHovWJwRWVX15CQVpUXsVw8boUsvOL5rULIZTGyPZiWv/SXFx6DDi0qHV1iL134nDnd3I+HvnUCHvuOMwmUhTVY6GVh8L7xf373ZJx5VHulYwPuucNVCpvIXp6ZtOj5FLpe8onrZAOmFqK4tz4/z/Z97JRVAIDbh/YOpaj5TKoPXKY+mM/jsOiF3zOUu+f/9O5KvOcSFuzHkt9e6Fh2dOfUuNQk7sXNz4wVUxvngqgt+m+dHHwMLAhFp+hl53+n6S6Y+auhjnWiQq5vTCgNtFhZDa2EUyJeFqr19o7HyDaVu7uZwOu2c3tj7t3nOywl2W87UNuAzzfsxoeKyZG+fWp3Wy4ZEcsv6pXe9XtnHJn8fG7fTjitp7qid0PF4yA7p5ns9fAKTtTdvCjiC6h5pXtmEfE3iL1GKzX0/HW7pAOlQZC5flTwG4zlXRv//vgLpXkeIqf3aif9fc25tMXWMwbYB/a9xhciwePxX7/zIHqMnoipIV9ubhwVcLJgUJQUPRENJ6KVRFRFRKMl6yuJaLy5/jMi6iGsP4KI9hPRL6IR2x2Zkj7zKGPQUTaVW3xI/z6tCi99ts6xnYjlunGLrvFU9Fy9TOuhatnEqRzc3EI8t784H9/4+ye+28m49swejmVW8jCvcMqdnJ+9SXkcw/p1Vj6mW74hFdfNpQO6AjAmV1lEnfPELhP/2T3qpixGuHvEsdw69zb9fuaQo42xpG+ceHjaibuOOzzcnAzxmRAVfxReBrfxJzEgwYJ3bYTJMR8EL0Nv8EPTARhpNiI9ZoYLmfjeSUQUB/AYgIsA9ANwFRH1Eza7AcAuxlhvAGMB/FFY/xcAk9MX1x/xfHVoUeHZvRdP8IRFm/Coy+w1Hmviw6bdKT/uQ1zKYa+slNZNWxZPFbOQFQ7xs1z21tRj+kq5JX/yEW089wWcCaUA4MzeHVA9ZkSydyFDVHpu4x8ylmx0lo0D1FwwV59u9CT4l5AsMiUqeBeG6M7gxSWCLRlbet4k4z6oLIsFUvSH6qKLw3a4ooSXaRRuC7fr7fbibuQicJqUZ9YRoRZeGQ0LfjMMgH/hoHRROWODAFQxxtYyxuoAvAxgpLDNSADjzM+vATiPTJOHiL4J4AsAS6MRWZ3qMSMw9+5hntuEHQSpqW/E5j2HbANGrTirXMV1w/vo3UqneXH3G+6z6V7/4VmoHjPCc/9rzujhWMb7l0838+60FFwRP/SZuJQpendqgaevHYgHOd/zke3tvZ4oLSNeGXlF3XxStQNtmlVg8k8GAzBSK7vh546ZvsJ4cZfFYoHGH0b8zZk/JSyisZ2JcZAg6SYA+/Nxy5Do77/5vxmGt390NoDsDsZaeqJqW+5z3XQFwM9U2WAuk27DGGsAsAdAeyJqAeD/APzW6wBEdDMRzSWiudu2pZeIP+j5D3vB5q/bjTMenJb83qZZuS3u9/EP12D2F/JolVrTfVFRFktaMG7jxbzF2ksoPLF1X3ql5Q6TDPbxPZG7Rxgdt65t7S6vToolEIOw4itnuTkZQ/t2tpWfE8dYogzBU0lyxnNsl1aoHjMCpxzpXgnKT2lakT7ixKgYAb+44GiMvdIZiQOkQlmjKE4iWvTiOXa7r4MgZrTkmXPX+Y5ljQmGI9s3Q59OLfDtUz2qjoWkXfMKdDDTVLsmhpuamqEcRa/mkVEngczLzKcDyQSZHoy9D8BYxpjn64ox9iRjbCBjbGDHjv65oH3aSmt/kcF9OmDu3c4bT+TFG0+3Pfwbdh3Ct5+YKd02WUatvMzXoue7sm2aldsmaEX1U7txipx/qKOIyc40x3e1+6HD9Izc8Irtz/SZEUMaTz6iLW4f2gcjTxRtrBTTV2xNpgkWQ3WD4JgwJbgh3fK1REU7ScKyhgRDRTymXFglDH63+9ipq5Kfg9oTYmTaizeehpEndc3aM6ai6DcC4F+h3cxl0m2IqAxAawA7AJwG4CEiqgZwB4BfE9HtacrsiXX+xdQGKrSVPBwfrd6uNPgTxE9t+VObVcSTivVsl8HPCu4hm79uN77z788wfaWRA0RVp53V2z0qZu0DF2PGL89NfucHGa2P6erOO4cHr9upyvDjD8PjV6fCRbNl0aej6efcdT7++V33EFcAjjh86wXmNT/jtfmpKkbv3jEktHziMYJM3MsUX+2pMVJCZyENuVr2ymD3mbj9mebznoWfYxxHYZs5APoQUU8iqgAwCsAEYZsJAK4xP18OYBozGMwY68EY6wHgrwAeYIz9PSLZpVjnUxZRAqSy3j17/SBbpMRT1wzEW6aPTkTVR6ma+3yHGbXSrCIOIsL0X3wNj199inRb2e103TNzsOdgvesNyXfvV/x+OMZdN8hVlliMEIuRI98HwCl6l+PcNLina7sWk38yGG8vinYik0i3tqmB43Me/iDw/rKJSYC3hZdO3pWOLStRtdXbJ2tZ0S/edBqAYIN18RihcxruNT/XTS6YtmIrEgmmNN8iLFbLd3FjX/tq6rFlbw321fiXfJRhJStLJFIZYW2hufli0Zs+99sBvAtgOYBXGGNLieh3RPQNc7OnYPjkqwD8DIAjBDN7GBfALffLf39wJp783ikYcnRH3Di4F6b8dAgeGXUSzju2s01h8PhdjLduP9s8pnPdmws34pyHp9vCMG9/cT6AVLqFnh2aSyNgAGOqv4znZlW7dh8vHZDap0l53HWWJs/YK0+yFdEGgPoGa9DYuX/1mBG4a4QYfAW8//NzbN+P7dIKW/amN5bgBx+aqlLST+SyAXJ3SEOj+8Oc7gBljU/SOyunTXfznjypeyqK6p07Bnvum67qyHTelSD8buRxyc8NCZZZ2bim31++BRt3H0L/+97DaQ+8j7PGTLNtqqLnX/xsHfr+5h38/u1laDQL1R/ftVUysRwQfR0JN5RqxjLGJgGYJCy7h/tcA+AKnzbuCyFfaNzO32Gtm+Cw1qmoiD6dW6JPZ+8skX43V08z3l221U9eXggA2Lq3NhmyWO+hQESuPv1Iabhn04qyyCNM4jH7y6ZDS8NX6jXBSuSojk4faqum5cleTCYQI294rhwoH7h79KoByYk+bg+bV3hjkPJ6YbCs6O7tmmHyTwbbzmt3F4PEIl3lIRo2bSIq8vH41Sfj1ufnK237zh2DESPC0Z1b4p43jYC9DbsO4fRembToU23fMG4urjurR/K7WOVN5dn79RuLAQBPffwFiAz37Ns/sr+k88aiLzQyMe/Az4iwHkqvazbvy12obWjElGVbkpOSTleYUeoW4dKldROpQ8XN1x+GLq2bYt7d5yuHU/JVingyPcEFgGsmzvIy+UXhXVVuvb9fXHiM0mD8UJc0GOnAGxfHdmlle+m4zbyNSmWIho3MXXJ46yauqS+s6BV+7OS+S/ph+PFd8KuL+irJ0PewVrZ0CF6yRIXYq16zzb1GL69n3lokT2w4gqsuxphcqWtFHxLr/Ic9f7+4wJl10C8FsBUZ47XdHeMX4gfPz8dNz87Fsk17UVkWS2tgKR4jqevmmetODd2mjPYtKpVSIC+8ZxheuPE06brKDE9wAezVrHiC1IyVtfncDaclFZcbT3xPPr7ihZdc5XEKVXErKrxSMlts2lPjWuN3u1kWr0OLyuRLY8ARhhFwrWklPzLqpHCyZdB101xQ9DM80orwPvofvbRAmq32xO72iDCZ6Pk0GFtQWOc/7AN++9A+gfexbma/I05bYUTLLNqwJ1CI2vDjnBNwymIk7b5kMiWAF22aVSRn8lq9CiuFQ5M0c7aocN83jpMu93qQjuncEqf1dI95VyXqc64S5XLO0e5RZem83ABJcRSX5tw6z9efZQzSH9ulVTIKyjpHlWVxVI8ZgZEnuYeJepHJ2zvIy1U0svrcNRnTV2y1vRxE3f/Ramftimy90ItQ0VuDsTk4dobafVxiMcZilNGKSjJm/eo8aWI4kY/NWH9rEs9ohe56C49EYCp0dEng5fUgvfvTIRgvFi8JyGUnh1NYLSS5jSxUolzGXT/IVsAd4EJLI7j3+ZqssnNYHidXN+ldI47Fgt8Ms7mYKlxcaCqseeDipPsvk66bIMiibq77zxx8/+nZSetejMzKdCpiL9J7uvKQpOsmjTY+uvNcHKpvxAVjZ/huy/uv3XzUGYEBn2/Y479dhMhm0qpwYnf/vDt//NYJvtvkG34pJrzwGrfYp1gn4FCdfbvJS74CEI2v/q+jTkqWN5T5kc84qoMj7PamwT1xWs/2iMcIbYUB3Ip4+F5dPEZoVhHHofrGrMTRq+D1vtl9sB4dW1aitqERZTFKKngrVDYXFJ1FnySN+6F7u2ZJ/6PfDEM+d0mHFpVpPfxBUC0akgssa/BSl7BFGbLJakFZff9FuEDIpOlXuzdXRBExdbckvDUq+BTH63YcSMaAWzDG0Lml/cV/14h+ON8lk6nboLgqTfLMovdyj42fsw57DtXjHx+sQUOCYdUfLsKiey5IZtEVaVFZhnNDTPAMQtEp+qiibogIf73yJLx521nS9Sd2MwZasjHQCAA/G2YfJH5ixhrHNoMi8DdHwfhbTsfT1w7E2CvVB9xO65V+TvvyeAyPX30K/v39gclltfWZna4flksjKDTRo0Nz/N9wtSiWoPBW/JrtBzD+ljNsabMZA24NkNwu3XEM6znLlxh/r1Qbf3pvFU787XvJ7xVlMbT2MGS6tW2adkpqP4pP0VsTpiLowH5zQFfXGG0rFt6vWLMbJ3QLliv8x+fZB4llCixPngF0a9sMQ/uq56j/+3cGRPYAx2KEfbWpWYxRVwKKiqjSK8tOWxRGL+8iYYyhRWWZLX6/R4dmKI/HlNxyQPqKvsZMG6JaYCcq+FKVPOnMjJaRafd90Sl6y22Y6R6epUDCvolPUnxAePgSfjKLwop2yEe88t18/QRn+oV02HkgpegDzE2TFn/JdzJl4fLNWrfax1xCvV+bhezrFRVeZZoW66Y9xuxq1SynYeFr1gLAVacdYfvuNuifDjFyH9iO7BiZbT77RDEYq0IqbMx5pKk/OwcL7xnmSCnAE8a/yo/aH5D46Id4hNzlBx7T0AAADpxJREFUmh9+rTfuu6Qfnr3ePe9OVPC9pUsUZ/VO/PHZjvQN2UKlSIwbq7c4c+bUROCu4l03lhKy7vlrz+yRTBXtVY8ZSI1x5UO+HBVuONtuLF192hG2+QITfyzPh+WGyphdLJYHFaYKjWQcfYZN+l0HjSn9soiE3p1aJOPK/3zFiY58NdVjRoTqCfCZGbfsDZ7TJddce1bPrLyM+h6WmlF5gWQOgozjDm+NTi2jz7OvQqum5aEH8b0qmaUDf1+L7i8+rNPKB8SHY/K8/oMz8dtvHKeUbylfOI+b6UxE6NQqZcXLXG58ptBnrx/kOkvbDQJl3MVYOGc/IFHr+e7tmuL8Y1N+ZyvqpdJnev+3Tulmqzh0dOfw+bS9KhcBuZk7kI8UQh59IBWVZBXEtip6BcGrEHk6yAzwv155Erq2aYoBnNvRqq3w8s2nS9vp1bEFrnHJJBuGnw9zzlyPmh+eawwyWz2tQT1SgQKyMbnKshhuM/c57vBWeO6G0/Ds9YMwT6GOBWCc60yPJBWdolfJJR2Gj+4cin9fMxC3DOkFIPVwdmvrn9yKf1m7pSNW4cHL+uOZa91THORD3vB8oFAU/ZCjDcvPssrvOD+4EsvUL+V9/1ZH8psDuuKT0UNtA7X3XtIPXds0RZeQcyxUOcbMe3PLOZkvY3lsl1bo06kF7jLTmN8+tHdyHRHhFWGSHRHwywv7onrMCLQ302UMObpj8rMvJE9nEiVFpxlSKRAywy8vPAbTf/G1ZKUbsZ6qVCbz//nHdkIvSXZHVcrjMZzrkUArX0LPck2B6Plk+UjxxXRke+/slDYy9Ft51+cvL3QfSB9+fBd8Mnpoxl0zL998Op69flDGwxABoFlFGab87JxkSch4jHB811T0zaCe7fDYd1IJ25QVugtGNpPMavrCCzPwId2kZn6UxWPo2aE5Hhk1AG8t2qRU2syaBdm+efQj9g9c2j+ZDlVjUCgWfUJI12GNwRwWoGhIFGHEMnijwS15WTZp27wip8EGr9xyBg7UpibfjTihC0acEM3kyGxE3RSfomfZibs5rHUT3GS6cfw4q3d7/H7kcbgsgkkyADD71+dh0APvAwCG9euMwX06ZMXSKRQKpWNjKXYrIsUqMRmk15epdxp/DvMl7UAuaVZRZitKHyWEzM/3KD5Fb/7PJ6OOiPC9M3pE1h6fo74sRsmCJoXCyzefjq5tmmLwQ9Mz0n6hWPT1lqI33R7n9u2E0Rf1xffPOFK5jUz90kI5h8WAtujToFRu0yZZKOoRNadHkO7Ai0LRUQ1mlsNy02KOxwi3BhxszFRRF63oswdR5i364uvv5+eM98ixkne51ZotZXJZtCMIPTsY6TX6dws/YeqmIb1w7yXRJzfTA/vZg+SlJSKl6Cx65lMcvFj459WnSKvaFBKfjB6aMX/6ZSd3DZQ9Mxd87ZhOmPLTIUoD+m40KY/jurN64rdvLUsue/jy9FM+a0WfPWJEaERmn+XiU/QZDq/MF2TFvAsNt9mUUfCXb4crVZdt/ArTh+EKl4LomvwkpuPog5NKgZBbOTQajUYF7aMPQSq4Umt6Tenwt6sGRN5ml9ZNcFRHeZpuTXSQjroJj7boNaXEJScejtH//RwX91fL1qnCzF+dF1lbGnf0zNgQZPqEaTT5ytLfuafF1uQvxoSpzB6jaF03Go1GUwjEiDKWjDF5jIy2ngP0YKxGoykkiAg+9VvSpugUPSKsGavRaDSZJm+ibohoOBGtJKIqIhotWV9JROPN9Z8RUQ9z+TAimkdEi83/Q6MV34m26DUaTSGRjblpvoqeiOIAHgNwEYB+AK4iInHO9Q0AdjHGegMYC+CP5vLtAC5hjPUHcA2A56IS3A+t6DUaTSGQL6UEBwGoYoytZYzVAXgZwEhhm5EAxpmfXwNwHhERY2wBY2yTuXwpgKZEFH1Sdg49GKvRaAqJWCw/om66AljPfd9gLpNuwxhrALAHgJii8FsA5jPGHFWtiehmIppLRHO3bdumKruUVAoEbdJrNJr8x5gwlXuLPm2I6DgY7pxbZOsZY08yxgYyxgZ27JheFZlUUrO0mtFoNJqsQMh89koVRb8RAJ8lqZu5TLoNEZUBaA1gh/m9G4A3AHyfMbYmXYH9KJWkZhqNpjgwkprl3qKfA6APEfUkogoAowBMELaZAGOwFQAuBzCNMcaIqA2AiQBGM8Y+iUpoL/KxwpRGo9G4EaPMjy36KnrT5347gHcBLAfwCmNsKRH9joi+YW72FID2RFQF4GcArBDM2wH0BnAPES00/zpF/iukaE2v0WjyH8qCRa+U64YxNgnAJGHZPdznGgBXSPb7A4A/pCljIHSuG41GU0gQQc+MDYt23Wg0mkIgG/V5i07R68FYjUZTSBjZK3M/GFtQlErNWI1GUxzEslB4pPgUvbboNRpNAWHMjNUWfSi0Qa/RaAoDXRw8MDroRqPRFBJG9kpt0QdCFwfXaDSFhDEzNsPHyGzz2ceKo9euG41GUwjkTeGRQkJ7bjQaTSGho27CoCtMaTSaAkNb9CHRcfQajaYQ0BZ9CJh23mg0mgIiRpnP0VV8il5PmNJoNAWEMRib2WMUn6I3/2vPjUajKQTypfBIQaFrxmo0mkKCiHJfeKTQ0DVjNRpNIUHaRx8cy9eVjRzPGo1Gky4x7aMPTmOjUaqlLKYVvUajyX+M8Ept0QeiwXw1xuNa0Ws0mvzHKDyS2WMUraLXFr1GoykErMmdmbTqlYqDFwIrvtqLH724ADsP1AEA4lrRazSaAsAaTxw2dgbOPaYj7hrRL/JjFI2ib1IWR5/OLQAAPTs0R2VZPMcSaTQajT8XHNcZq7fuQ4IxdG7VJCPHoEwPAgRl4MCBbO7cubkWQ6PRaAoKIprHGBsoW1d0PnqNRqPR2NGKXqPRaIocreg1Go2myNGKXqPRaIocJUVPRMOJaCURVRHRaMn6SiIab67/jIh6cOt+ZS5fSUQXRie6RqPRaFTwVfREFAfwGICLAPQDcBURiYGeNwDYxRjrDWAsgD+a+/YDMArAcQCGA/iH2Z5Go9FosoSKRT8IQBVjbC1jrA7AywBGCtuMBDDO/PwagPPImO41EsDLjLFaxtgXAKrM9jQajUaTJVQUfVcA67nvG8xl0m0YYw0A9gBor7ivRqPRaDJIXsyMJaKbAdxsft1PRCvTaK4DgO3pS5VRtIzRoGWMBi1jNORaxiPdVqgo+o0AunPfu5nLZNtsIKIyAK0B7FDcF4yxJwE8qSCLL0Q01212WL6gZYwGLWM0aBmjIZ9lVHHdzAHQh4h6ElEFjMHVCcI2EwBcY36+HMA0ZuRWmABglBmV0xNAHwCzoxFdo9FoNCr4WvSMsQYiuh3AuwDiAJ5mjC0lot8BmMsYmwDgKQDPEVEVgJ0wXgYwt3sFwDIADQBuY4w1Zui3aDQajUaCko+eMTYJwCRh2T3c5xoAV7jsez+A+9OQMSiRuIAyjJYxGrSM0aBljIa8lTHvsldqNBqNJlp0CgSNRqMpcrSi12g0miKnaBS9Xz6eLMtSTUSLiWghEc01l7UjoilEtNr839ZcTkT0qCn350R0coZkepqIthLREm5ZYJmI6Bpz+9VEdI3sWBHLeB8RbTTP5UIiuphbJ82jlMl7gYi6E9F0IlpGREuJ6Cfm8rw5lx4y5s25JKImRDSbiBaZMv7WXN6TjHxZVWTkz6owl2c9n5aHjP8hoi+483iSuTwnz40SjLGC/4MRDbQGQC8AFQAWAeiXQ3mqAXQQlj0EYLT5eTSAP5qfLwYwGUYx+NMBfJYhmYYAOBnAkrAyAWgHYK35v635uW2GZbwPwC8k2/Yzr3MlgJ7m9Y9n+l4A0AXAyebnlgBWmbLkzbn0kDFvzqV5PlqYn8sBfGaen1cAjDKXPw7gB+bnHwJ43Pw8CsB4L9kzLON/AFwu2T4nz43KX7FY9Cr5eHINnw9oHIBvcsufZQazALQhoi5RH5wxNgNG6Gs6Ml0IYApjbCdjbBeAKTCS1WVSRjfc8ihl9F5gjG1mjM03P+8DsBxGWo+8OZceMrqR9XNpno/95tdy848BGAojXxbgPI9ZzaflIaMbOXluVCgWRZ9vOXUYgPeIaB4Z6R0AoDNjbLP5+SsAnc3PuZQ9qEy5kvV2syv8tOUSyQcZTffBABiWXl6eS0FGII/OJRHFiWghgK0wlN8aALuZkS9LPF5O8mmJMjLGrPN4v3kexxJRpSijIEvO9VOxKPp842zG2MkwUjvfRkRD+JXM6M/lVVxrPspk8k8ARwE4CcBmAH/OrTgGRNQCwH8B3MEY28uvy5dzKZExr84lY6yRMXYSjNQogwD0zaU8MkQZieh4AL+CIeupMNwx/5dDEZUoFkWvlFMnWzDGNpr/twJ4A8ZNvMVyyZj/t5qb51L2oDJlXVbG2BbzYUsA+BdS3fKcyUhE5TAU6AuMsdfNxXl1LmUy5uO5NOXaDWA6gDNguDusiZz88ZKyUIh8WhHKONx0jTHGWC2AZ5An59GLYlH0Kvl4sgIRNSeiltZnABcAWAJ7PqBrALxpfp4A4PvmiP3pAPZwLoBME1SmdwFcQERtzW7/BeayjCGMV1wK41xaMsryKGX0XjD9wk8BWM4Y+wu3Km/OpZuM+XQuiagjEbUxPzcFMAzGWMJ0GPmyAOd5zGo+LRcZV3AvdIIxhsCfx7x4bhxkc+Q3k38wRrxXwfDz3ZVDOXrBiAJYBGCpJQsMf+L7AFYDmAqgHUuN7D9myr0YwMAMyfUSjO56PQwf4Q1hZAJwPYwBryoA12VBxudMGT6H8SB14ba/y5RxJYCLsnEvADgbhlvmcwALzb+L8+lcesiYN+cSwAkAFpiyLAFwD/f8zDbPyasAKs3lTczvVeb6Xn6yZ1DGaeZ5XALgeaQic3Ly3Kj86RQIGo1GU+QUi+tGo9FoNC5oRa/RaDRFjlb0Go1GU+RoRa/RaDRFjlb0Go1GU+RoRa/RaDRFjlb0Go1GU+T8P2gTWB1DOZuGAAAAAElFTkSuQmCC\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["convertedArray = [convertedArray1, convertedArray2, convertedArray3]"],"metadata":{"id":"_ezB8dl0SA12"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.boxplot(convertedArray)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":647},"id":"SDOKa3HKSEF3","executionInfo":{"status":"ok","timestamp":1650196624883,"user_tz":240,"elapsed":483,"user":{"displayName":"Raima Sen","userId":"00263744560502469226"}},"outputId":"37ff0c61-938b-415a-f25c-b6a0f1ea04ea"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'boxes': [<matplotlib.lines.Line2D at 0x7f85edd3ba50>,\n","  <matplotlib.lines.Line2D at 0x7f85edccf190>,\n","  <matplotlib.lines.Line2D at 0x7f85edce0690>],\n"," 'caps': [<matplotlib.lines.Line2D at 0x7f85edd3ea90>,\n","  <matplotlib.lines.Line2D at 0x7f85edd3efd0>,\n","  <matplotlib.lines.Line2D at 0x7f85edcd6150>,\n","  <matplotlib.lines.Line2D at 0x7f85edcd6690>,\n","  <matplotlib.lines.Line2D at 0x7f85edce96d0>,\n","  <matplotlib.lines.Line2D at 0x7f85edce9c50>],\n"," 'fliers': [<matplotlib.lines.Line2D at 0x7f85edcc6ad0>,\n","  <matplotlib.lines.Line2D at 0x7f85edce0150>,\n","  <matplotlib.lines.Line2D at 0x7f85edcf0710>],\n"," 'means': [],\n"," 'medians': [<matplotlib.lines.Line2D at 0x7f85edcc6590>,\n","  <matplotlib.lines.Line2D at 0x7f85edcd6bd0>,\n","  <matplotlib.lines.Line2D at 0x7f85edcf01d0>],\n"," 'whiskers': [<matplotlib.lines.Line2D at 0x7f85edd3bfd0>,\n","  <matplotlib.lines.Line2D at 0x7f85edd3e550>,\n","  <matplotlib.lines.Line2D at 0x7f85edccf6d0>,\n","  <matplotlib.lines.Line2D at 0x7f85edccfbd0>,\n","  <matplotlib.lines.Line2D at 0x7f85edce0bd0>,\n","  <matplotlib.lines.Line2D at 0x7f85edce9190>]}"]},"metadata":{},"execution_count":7},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["#AREA for 1 frame car wash trial"],"metadata":{"id":"yWXLRhnuSiP4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x1 = df['DLC_resnet50_Video5Apr16shuffle1_5000'][2:].values.astype(np.float) # p left wrist\n","y1 = df['DLC_resnet50_Video5Apr16shuffle1_5000.1'][2:].values.astype(np.float)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QDaKqKwOzniB","executionInfo":{"status":"ok","timestamp":1650196672555,"user_tz":240,"elapsed":314,"user":{"displayName":"Raima Sen","userId":"00263744560502469226"}},"outputId":"3283ab3e-2381-441f-80d8-89782f9255d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  \"\"\"Entry point for launching an IPython kernel.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  \n"]}]},{"cell_type":"code","source":["x1 = x1[x1!=0]\n","y1 = y1[y1!=0]"],"metadata":{"id":"cDZcvccQ1BHr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#y.shape"],"metadata":{"id":"3mFjEoP31iU1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["p_left_wrist_coords = np.vstack((x1,y1)).T\n","print(p_left_wrist_coords.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nw7PAnEH0aMz","executionInfo":{"status":"ok","timestamp":1650196701220,"user_tz":240,"elapsed":308,"user":{"displayName":"Raima Sen","userId":"00263744560502469226"}},"outputId":"ab989095-3efe-452f-b144-ad5c68b4eb84"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(3703, 2)\n"]}]},{"cell_type":"code","source":["x2 = df['DLC_resnet50_Video5Apr16shuffle1_5000.3'][2:].values.astype(np.float) # p right wrist \n","y2 = df['DLC_resnet50_Video5Apr16shuffle1_5000.4'][2:].values.astype(np.float)\n","\n","x3 = df['DLC_resnet50_Video5Apr16shuffle1_5000.6'][2:].values.astype(np.float) # p left elbow\n","y3 = df['DLC_resnet50_Video5Apr16shuffle1_5000.7'][2:].values.astype(np.float)\n","\n","x4 = df['DLC_resnet50_Video5Apr16shuffle1_5000.9'][2:].values.astype(np.float) # p right elbow\n","y4 = df['DLC_resnet50_Video5Apr16shuffle1_5000.10'][2:].values.astype(np.float)\n","\n","x5 = df['DLC_resnet50_Video5Apr16shuffle1_5000.12'][2:].values.astype(np.float) # p left shoulder\n","y5 = df['DLC_resnet50_Video5Apr16shuffle1_5000.13'][2:].values.astype(np.float) \n","\n","x6 = df['DLC_resnet50_Video5Apr16shuffle1_5000.15'][2:].values.astype(np.float) # p right shoulder\n","y6 = df['DLC_resnet50_Video5Apr16shuffle1_5000.16'][2:].values.astype(np.float)\n","\n","x7 = df['DLC_resnet50_Video5Apr16shuffle1_5000.18'][2:].values.astype(np.float) # p neck\n","y7 = df['DLC_resnet50_Video5Apr16shuffle1_5000.19'][2:].values.astype(np.float)\n","\n","x8 = df['DLC_resnet50_Video5Apr16shuffle1_5000.24'][2:].values.astype(np.float) # t left wrist\n","y8 = df['DLC_resnet50_Video5Apr16shuffle1_5000.25'][2:].values.astype(np.float)\n","\n","x9 = df['DLC_resnet50_Video5Apr16shuffle1_5000.27'][2:].values.astype(np.float) # t right wrist\n","y9 = df['DLC_resnet50_Video5Apr16shuffle1_5000.28'][2:].values.astype(np.float)\n","\n","x10 = df['DLC_resnet50_Video5Apr16shuffle1_5000.30'][2:].values.astype(np.float) # t left elbow\n","y10 = df['DLC_resnet50_Video5Apr16shuffle1_5000.31'][2:].values.astype(np.float)\n","\n","x11 = df['DLC_resnet50_Video5Apr16shuffle1_5000.33'][2:].values.astype(np.float) # t right elbow\n","y11 = df['DLC_resnet50_Video5Apr16shuffle1_5000.34'][2:].values.astype(np.float)\n"," \n","x12 = df['DLC_resnet50_Video5Apr16shuffle1_5000.36'][2:].values.astype(np.float) # t left shoulder\n","y12 = df['DLC_resnet50_Video5Apr16shuffle1_5000.37'][2:].values.astype(np.float)\n","\n","x13 = df['DLC_resnet50_Video5Apr16shuffle1_5000.39'][2:].values.astype(np.float) # t right shoulder\n","y13 = df['DLC_resnet50_Video5Apr16shuffle1_5000.40'][2:].values.astype(np.float)\n","\n","x14 = df['DLC_resnet50_Video5Apr16shuffle1_5000.42'][2:].values.astype(np.float) # t neck\n","y14 = df['DLC_resnet50_Video5Apr16shuffle1_5000.43'][2:].values.astype(np.float)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lYc8R_E61RVY","executionInfo":{"status":"ok","timestamp":1650196984528,"user_tz":240,"elapsed":284,"user":{"displayName":"Raima Sen","userId":"00263744560502469226"}},"outputId":"a7677bc3-955f-4e51-f710-23f88460881a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  \"\"\"Entry point for launching an IPython kernel.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  \n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  after removing the cwd from sys.path.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  \"\"\"\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  import sys\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  \n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:10: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  # Remove the CWD from sys.path while we load stuff.\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  # This is added back by InteractiveShellApp.init_path()\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  del sys.path[0]\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:14: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  \n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:16: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  app.launch_new_instance()\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:17: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:19: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:20: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:22: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:23: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:25: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:26: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:32: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:34: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:37: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n"]}]},{"cell_type":"code","source":["x2 = x2[x2!=0]\n","y2 = y2[y2!=0]\n","\n","x3 = x3[x3!=0]\n","y3 = y3[y3!=0]\n","\n","x4 = x4[x4!=0]\n","y4 = y4[y4!=0]\n","\n","x5 = x5[x5!=0]\n","y5 = y5[y5!=0]\n","\n","x6 = x6[x6!=0]\n","y6 = y6[y6!=0]\n","\n","x7 = x7[x7!=0]\n","y7 = y7[y7!=0]\n","\n","x8 = x8[x8!=0]\n","y8 = y8[y8!=0]\n","\n","x9 = x9[x9!=0]\n","y9 = y9[y9!=0]\n","\n","x10 = x10[x10!=0]\n","y10 = y10[y10!=0]\n","\n","x11 = x11[x11!=0]\n","y11 = y11[y11!=0]\n","\n","x12 = x12[x12!=0]\n","y12 = y12[y12!=0]\n","\n","x13 = x13[x13!=0]\n","y13 = y13[y13!=0]\n","\n","x14 = x14[x14!=0]\n","y14 = y14[y14!=0]"],"metadata":{"id":"VkynHGoE4jU2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["p_right_wrist_coords = np.vstack((x2,y2)).T\n","p_left_elbow_coords = np.vstack((x3,y3)).T\n","p_right_elbow_coords = np.vstack((x4,y4)).T\n","p_left_shoulder_coords = np.vstack((x5,y5)).T\n","p_right_shoulder_coords = np.vstack((x6,y6)).T\n","p_neck_coords = np.vstack((x7,y7)).T\n","\n","t_left_wrist_coords = np.vstack((x8,y8)).T\n","t_right_wrist_coords = np.vstack((x9,y9)).T\n","t_left_elbow_coords = np.vstack((x10,y10)).T\n","t_right_elbow_coords = np.vstack((x11,y11)).T\n","t_left_shoulder_coords = np.vstack((x12,y12)).T\n","t_right_shoulder_coords = np.vstack((x13,y13)).T\n","t_neck_coords = np.vstack((x14,y14)).T\n"],"metadata":{"id":"xgsZ7PBv_Hjw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import cv2"],"metadata":{"id":"0ZfywNS4CXnJ"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":[],"name":"car_wash_with_CSV","provenance":[{"file_id":"https://github.com/DeepLabCut/DeepLabCut/blob/master/examples/COLAB/COLAB_YOURDATA_TrainNetwork_VideoAnalysis.ipynb","timestamp":1639191884460}]},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}